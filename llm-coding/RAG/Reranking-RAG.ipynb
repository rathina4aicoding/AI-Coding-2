{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78ea4a90",
   "metadata": {},
   "source": [
    "## RAG with Re-Ranking (Better Relevance)\n",
    "\n",
    "How it works:\n",
    "Instead of blindly sending the top search results, a re-ranker (often another small ML model) scores results for relevance.\n",
    "Only the most relevant documents go to the LLM.\n",
    "Example Use Case:In healthcare, a clinical assistant retrieves only the most relevant medical guidelines for a doctor’s query instead of dumping too much text.\n",
    "\n",
    "User Query → Retriever → Re-Ranker (sort/filter results) → Top Docs → LLM → Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11586d23",
   "metadata": {},
   "source": [
    "High-level idea (what the app does)\n",
    "\n",
    "1. Load a PDF (default: Amazon Bedrock – User Guide.pdf).\n",
    "2. Split it into chunks → embed chunks → store in FAISS (a vector index).\n",
    "3. For a user question, retrieve a broad pool of chunks (top-N).\n",
    "4. Re-rank that pool with a Cross-Encoder (query+text scorer) → keep best K.\n",
    "5. Send those best K chunks as context to an LLM to generate a grounded answer.\n",
    "6. Show the answer + sources in a simple Gradio UI.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "214dde2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tempfile\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Tuple, Any, Dict\n",
    "\n",
    "import gradio as gr\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# LangChain\n",
    "from langchain.schema import Document\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.runnable import RunnablePassthrough, RunnableMap\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# re-ranking model.\n",
    "from sentence_transformers import CrossEncoder\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4481dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "!uv add sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11550992",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables in a file called .env\n",
    "\n",
    "load_dotenv(override=True)\n",
    "os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY', 'your-key-if-not-using-env')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c5f364",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_LOCAL_PDF = \"Amazon SageMaker AI-Developer Guide.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07608e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Purpose: Control how the LLM behaves.\n",
    "# What they say:\n",
    "# Be factual.\n",
    "# Use only the provided context.\n",
    "# If not present, say “I don’t know”.\n",
    "# Keep it concise and optionally show short citations like [source].\n",
    "# Why it matters: Keeps the LLM grounded and reduces hallucinations\n",
    "\n",
    "SYSTEM_PROMPT = \"\"\"You are a factual assistant. Answer ONLY using the provided context.\n",
    "If the answer is not present, say you don't know. Keep answers concise.\n",
    "Include short inline citations like [source] when helpful.\"\"\"\n",
    "\n",
    "PROMPT = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", SYSTEM_PROMPT),\n",
    "        (\"human\",\n",
    "         \"Question:\\n{question}\\n\\n\"\n",
    "         \"Context:\\n{context}\\n\\n\"\n",
    "         \"Answer using ONLY the context.\")\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "401d39e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Purpose: If Gradio returns raw bytes (instead of a file path), this makes a temporary .pdf file and returns its path.\n",
    "# How it works:\n",
    "# Creates a temp folder, writes the bytes to uploaded.pdf.\n",
    "# Returns that file path.\n",
    "# Why it matters: Later steps (PDF loader) need a filesystem path, not raw bytes.\n",
    "\n",
    "def _bytes_to_temp_pdf(file_bytes: bytes, suggested_name: str = \"uploaded.pdf\") -> str:\n",
    "    \"\"\"Write bytes to a temp .pdf and return the path.\"\"\"\n",
    "    tmp_dir = tempfile.mkdtemp()\n",
    "    pdf_path = os.path.join(tmp_dir, suggested_name)\n",
    "    with open(pdf_path, \"wb\") as f:\n",
    "        f.write(file_bytes)\n",
    "    return pdf_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ac49e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Purpose: Robustly figure out the actual PDF path to load.\n",
    "# How it works:\n",
    "# If use_default=True, ensure Amazon Bedrock - User Guide.pdf exists and return it.\n",
    "# Else, if user uploaded:\n",
    "# If it’s a string path (Gradio type=\"filepath\"), verify and return it.\n",
    "# If it’s bytes, call _bytes_to_temp_pdf(...) and return that path.\n",
    "# Otherwise, raise a helpful error.\n",
    "# Why it matters: No matter how the file arrives, the rest of the code always gets a valid path.\n",
    "\n",
    "def _resolve_pdf_path(pdf_input, use_default: bool) -> str:\n",
    "    \"\"\"\n",
    "    Returns a filesystem path to a readable PDF.\n",
    "    - use_default=True -> uses DEFAULT_LOCAL_PDF from current folder.\n",
    "    - pdf_input can be a filepath (str) or bytes (if input type changed).\n",
    "    \"\"\"\n",
    "    if use_default:\n",
    "        if not os.path.exists(DEFAULT_LOCAL_PDF):\n",
    "            raise FileNotFoundError(\n",
    "                f\"Default PDF not found: {DEFAULT_LOCAL_PDF} (place it in this folder or upload another file).\"\n",
    "            )\n",
    "        return DEFAULT_LOCAL_PDF\n",
    "\n",
    "    if pdf_input is None:\n",
    "        raise ValueError(\"Please upload a PDF or check 'Use default PDF'.\")\n",
    "\n",
    "    if isinstance(pdf_input, str):\n",
    "        if not os.path.exists(pdf_input):\n",
    "            raise FileNotFoundError(f\"Uploaded path not found: {pdf_input}\")\n",
    "        return pdf_input\n",
    "\n",
    "    if isinstance(pdf_input, (bytes, bytearray)):\n",
    "        return _bytes_to_temp_pdf(pdf_input)\n",
    "\n",
    "    raise TypeError(f\"Unsupported file input type: {type(pdf_input)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93cee35b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Purpose: Takes a list of retrieved chunks and turns them into a single readable context string with simple citations.\n",
    "# How it works:\n",
    "# Loops each Document d.\n",
    "# Builds labels like [Amazon Bedrock - User Guide.pdf (page 3)].\n",
    "# Concatenates their page_content into one string the LLM can read.\n",
    "# Why it matters: LLMs need a single text block of context; this function creates it and preserves where each chunk came from.\n",
    "\n",
    "def format_docs(docs: List[Document]) -> str:\n",
    "    \"\"\"Formats retrieved docs into a single string with simple citations.\"\"\"\n",
    "    lines = []\n",
    "    for i, d in enumerate(docs, start=1):\n",
    "        src = d.metadata.get(\"source\", f\"doc_{i}\")\n",
    "        page = d.metadata.get(\"page\", None)\n",
    "        page_str = f\" (page {page+1})\" if isinstance(page, int) else \"\"\n",
    "        lines.append(f\"[{src}{page_str}] {d.page_content}\")\n",
    "        print (src)\n",
    "        print (page)\n",
    "        print (page_str)\n",
    "        print (lines)\n",
    "    return \"\\n\\n\".join(lines)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb04bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Purpose: Pretty-prints which chunks/pages were used in the answer.\n",
    "# How it works:\n",
    "# For each Document, prints source (filename), page, and start_index (the character offset set by the splitter).\n",
    "# Returns a readable bullet list.\n",
    "# Why it matters: Learners see transparency—exactly which pages powered the answer.\n",
    "\n",
    "def show_sources(docs: List[Document]) -> str:\n",
    "    if not docs:\n",
    "        return \"No sources.\"\n",
    "    lines = []\n",
    "    for d in docs:\n",
    "        src = d.metadata.get(\"source\", \"unknown\")\n",
    "        page = d.metadata.get(\"page\", None)\n",
    "        start_idx = d.metadata.get(\"start_index\", \"?\")\n",
    "        page_str = f\"page {page+1}\" if isinstance(page, int) else \"page ?\"\n",
    "        lines.append(f\"- {src} ({page_str}), start_char={start_idx}\")\n",
    "        print (src)\n",
    "        print (page)\n",
    "        print (start_idx)\n",
    "        print (page_str)\n",
    "        print (lines)\n",
    "    return \"\\n\".join(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b2922e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Indexing\n",
    "# Purpose: Read the PDF and convert it to a list of LangChain Documents (one per page).\n",
    "# How it works:\n",
    "# Uses PyPDFLoader(pdf_path).load() → returns one Document per page, with metadata[\"page\"] set.\n",
    "# Adds metadata[\"source\"] = filename to each doc (used for citations).\n",
    "# Why it matters: Converts raw PDF into a structure that LangChain’s splitters and retrievers understand.\n",
    "\n",
    "def load_pdf_as_docs(pdf_path: str) -> List[Document]:\n",
    "    loader = PyPDFLoader(pdf_path)\n",
    "    docs = loader.load()  # each page is a Document with page metadata\n",
    "    for d in docs:\n",
    "        d.metadata[\"source\"] = os.path.basename(pdf_path)\n",
    "    return docs\n",
    "\n",
    "# Purpose: Build the vector search part of RAG.\n",
    "# How it works:\n",
    "# Split pages into overlapping chunks (RecursiveCharacterTextSplitter) so each chunk is a manageable size for embeddings/LLM.\n",
    "# Embed chunks with OpenAIEmbeddings.\n",
    "# Index them in FAISS: FAISS.from_documents(...).\n",
    "# Create a retriever view with k=initial_k—this is the broad candidate pool (e.g., top-15) returned for each question.\n",
    "# Return the retriever and some stats (pages, chunks, k, etc.) for the UI.\n",
    "# Why it matters: This is classic “RAG retrieval”: given a query, quickly find similar chunks by vector similarity.\n",
    "\n",
    "def build_vector_index(\n",
    "    docs: List[Document],\n",
    "    embed_model: str = \"text-embedding-3-small\",\n",
    "    chunk_size: int = 500,\n",
    "    chunk_overlap: int = 80,\n",
    "    initial_k: int = 15,\n",
    ") -> Tuple[Any, Dict[str, Any]]:\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size, chunk_overlap=chunk_overlap, add_start_index=True\n",
    "    )\n",
    "    chunks = splitter.split_documents(docs)\n",
    "\n",
    "    embeddings = OpenAIEmbeddings(model=embed_model)\n",
    "    vectorstore = FAISS.from_documents(chunks, embedding=embeddings)\n",
    "    retriever = vectorstore.as_retriever(search_kwargs={\"k\": initial_k})\n",
    "\n",
    "    stats = {\n",
    "        \"pages\": len(docs),\n",
    "        \"chunks\": len(chunks),\n",
    "        \"initial_k\": initial_k,\n",
    "        \"chunk_size\": chunk_size,\n",
    "        \"chunk_overlap\": chunk_overlap,\n",
    "        \"embed_model\": embed_model,\n",
    "    }\n",
    "    return retriever, stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0210b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Re-Ranker\n",
    "# Holds which Cross-Encoder model to use and how many chunks to keep (top_k) after re-ranking.\n",
    "# =========================\n",
    "@dataclass\n",
    "class RerankerConfig:\n",
    "    model_name: str = \"cross-encoder/ms-marco-MiniLM-L-6-v2\"\n",
    "    top_k: int = 5\n",
    "\n",
    "# Purpose: Ranks the candidate chunks using a cross-encoder (a model that reads the query and the chunk together and outputs a relevance score).\n",
    "# How it works:\n",
    "# __init__: loads the sentence-transformers cross-encoder once (CPU or GPU).\n",
    "# rerank(query, docs):\n",
    "# 1. Build (query, doc_text) pairs,\n",
    "# 2. self.model.predict(...) → get a score per pair,\n",
    "# 3. Sort by score descending,\n",
    "# 4. Return the top_k (Document, score) pairs (actually the code returns just Documents for the chain).\n",
    "# Why it matters: Plain vector similarity is fast but can be fuzzy. Cross-encoders are more precise because they read the query and full chunk together, giving a better final shortlist.\n",
    "\n",
    "class CrossEncoderReranker:\n",
    "    \"\"\"Re-ranks retrieved chunks using CrossEncoder scoring of (query, chunk).\"\"\"\n",
    "    def __init__(self, cfg: RerankerConfig):\n",
    "        self.cfg = cfg\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.model = CrossEncoder(cfg.model_name, device=device)\n",
    "\n",
    "    def rerank(self, query: str, docs: List[Document]) -> List[Tuple[Document, float]]:\n",
    "        if not docs:\n",
    "            return []\n",
    "        pairs = [(query, d.page_content) for d in docs]\n",
    "        scores = self.model.predict(pairs)  # numpy array\n",
    "        ranked = sorted(zip(docs, scores), key=lambda x: float(x[1]), reverse=True)\n",
    "        print (scores)\n",
    "        print (ranked)\n",
    "        return ranked[: self.cfg.top_k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "239b0f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================\n",
    "# RAG Chains\n",
    "# ===================================================\n",
    "\n",
    "# Purpose: A vanilla RAG chain without re-ranking.\n",
    "# Steps (Runnable pipeline):\n",
    "# Inputs: \"question\" (passthrough) + \"docs\" from retriever.\n",
    "# Build the prompt inputs:\n",
    "# \"context\" = format_docs(docs)\n",
    "# \"sources\" = the same docs\n",
    "# PROMPT → LLM → StrOutputParser to get plain text.\n",
    "# Output \"answer\" and \"sources\".\n",
    "# Why it matters: Lets learners compare with/without re-ranking.\n",
    "# ===========================================================\n",
    "def make_chain_no_rerank(retriever, model_name=\"gpt-4o-mini\", temperature=0.0):\n",
    "    llm = ChatOpenAI(model=model_name, temperature=temperature)\n",
    "    chain = (\n",
    "        RunnableMap({\"question\": RunnablePassthrough(), \"docs\": retriever})\n",
    "        | RunnableMap({\n",
    "            \"question\": lambda x: x[\"question\"],\n",
    "            \"context\": lambda x: format_docs(x[\"docs\"]),\n",
    "            \"sources\": lambda x: x[\"docs\"],\n",
    "        })\n",
    "        | RunnableMap({\n",
    "            \"answer\": ChatPromptTemplate.from_messages(\n",
    "                [(\"system\", SYSTEM_PROMPT),\n",
    "                 (\"human\", \"Question:\\n{question}\\n\\nContext:\\n{context}\\n\\nAnswer using ONLY the context.\")]\n",
    "            ) | llm | StrOutputParser(),\n",
    "            \"sources\": lambda x: x[\"sources\"]\n",
    "        })\n",
    "    )\n",
    "    return chain\n",
    "\n",
    "# Purpose: The RAG + Re-Ranking chain.\n",
    "# Steps:\n",
    "# Inputs: \"question\" and a broad pool of \"candidates\" from retriever (e.g., 15 chunks).\n",
    "# Re-rank: reranker.rerank(question, candidates) → keep top-K most relevant chunks.\n",
    "# Build the prompt inputs with those reranked chunks:\n",
    "# \"context\" = format_docs(reranked)\n",
    "# \"sources\" = reranked\n",
    "# PROMPT → LLM → StrOutputParser to get the final answer.\n",
    "# Output \"answer\" and \"sources\".\n",
    "# Why it matters: This is the “Better Relevance” upgrade—improves precision by giving the LLM fewer but better chunks.\n",
    "\n",
    "def make_chain_with_rerank(retriever, reranker: CrossEncoderReranker,\n",
    "                           model_name=\"gpt-4o-mini\", temperature=0.0):\n",
    "    llm = ChatOpenAI(model=model_name, temperature=temperature)\n",
    "    chain = (\n",
    "        RunnableMap({\"question\": RunnablePassthrough(), \"candidates\": retriever})\n",
    "        | RunnableMap({\n",
    "            \"question\": lambda x: x[\"question\"],\n",
    "            \"reranked\": lambda x: [d for d, s in reranker.rerank(x[\"question\"], x[\"candidates\"])],\n",
    "        })\n",
    "        | RunnableMap({\n",
    "            \"question\": lambda x: x[\"question\"],\n",
    "            \"context\": lambda x: format_docs(x[\"reranked\"]),\n",
    "            \"sources\": lambda x: x[\"reranked\"],\n",
    "        })\n",
    "        | RunnableMap({\n",
    "            \"answer\": PROMPT | llm | StrOutputParser(),\n",
    "            \"sources\": lambda x: x[\"sources\"]\n",
    "        })\n",
    "    )\n",
    "    return chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f144d106",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Gradio Callbacks\n",
    "# =========================\n",
    "\n",
    "# What it does:\n",
    "# 1.API key: use the textbox value if provided; else rely on env var.\n",
    "# 2.Resolve PDF path: _resolve_pdf_path(pdf_file, use_default).\n",
    "# 3.Load & index:\n",
    "# pages = load_pdf_as_docs(pdf_path)\n",
    "# retriever, stats = build_vector_index(...) using chosen chunk sizes, embeddings, and pool size (pool_k).\n",
    "# 4.Choose chain:\n",
    "# If use_rerank=True, construct a CrossEncoderReranker with rerank_model and rerank_top_k, then call make_chain_with_rerank(...).\n",
    "# Else, call make_chain_no_rerank(...).\n",
    "# 5.Build a short summary string (pages, chunks, k, models) for the UI.\n",
    "# 6.Return:\n",
    "# chain (stored in a hidden Gradio State)\n",
    "# index_summary (Markdown)\n",
    "# status message.\n",
    "# Why it matters: One-time setup per PDF; afterwards users can ask many questions.\n",
    "\n",
    "def ui_build_index(api_key: str,\n",
    "                   pdf_file,\n",
    "                   use_default: bool,\n",
    "                   model_name: str,\n",
    "                   temperature: float,\n",
    "                   embed_model: str,\n",
    "                   chunk_size: int,\n",
    "                   chunk_overlap: int,\n",
    "                   pool_k: int,\n",
    "                   use_rerank: bool,\n",
    "                   rerank_model: str,\n",
    "                   rerank_top_k: int):\n",
    "    try:\n",
    "        # API key\n",
    "        if api_key:\n",
    "            os.environ[\"OPENAI_API_KEY\"] = api_key\n",
    "        if not os.getenv(\"OPENAI_API_KEY\"):\n",
    "            raise RuntimeError(\"Missing OPENAI_API_KEY. Provide it in the textbox or environment.\")\n",
    "\n",
    "        # Resolve PDF path\n",
    "        pdf_path = _resolve_pdf_path(pdf_file, use_default)\n",
    "\n",
    "        # Load docs & build index\n",
    "        pages = load_pdf_as_docs(pdf_path)\n",
    "        retriever, stats = build_vector_index(\n",
    "            pages, embed_model=embed_model,\n",
    "            chunk_size=chunk_size, chunk_overlap=chunk_overlap,\n",
    "            initial_k=pool_k\n",
    "        )\n",
    "\n",
    "        # Choose chain (with or without reranking)\n",
    "        if use_rerank:\n",
    "            reranker = CrossEncoderReranker(RerankerConfig(\n",
    "                model_name=rerank_model, top_k=rerank_top_k\n",
    "            ))\n",
    "            chain = make_chain_with_rerank(retriever, reranker,\n",
    "                                           model_name=model_name, temperature=temperature)\n",
    "        else:\n",
    "            chain = make_chain_no_rerank(retriever,\n",
    "                                         model_name=model_name, temperature=temperature)\n",
    "\n",
    "        summary = (\n",
    "            f\"✅ Index ready for **{os.path.basename(pdf_path)}**\\n\"\n",
    "            f\"- Pages: {stats['pages']} | Chunks: {stats['chunks']}\\n\"\n",
    "            f\"- Retriever pool k: {stats['initial_k']} | \"\n",
    "            f\"Re-rank: {'ON' if use_rerank else 'OFF'} (top_k={rerank_top_k if use_rerank else '-'})\\n\"\n",
    "            f\"- Chunk size/overlap: {stats['chunk_size']}/{stats['chunk_overlap']}\\n\"\n",
    "            f\"- Embeddings: {stats['embed_model']} | LLM: {model_name} (T={temperature})\"\n",
    "        )\n",
    "        return chain, gr.update(value=summary, visible=True), \"Index built successfully.\"\n",
    "    except Exception as e:\n",
    "        return None, gr.update(value=\"\", visible=True), f\"❌ Error: {e}\"\n",
    "\n",
    "\n",
    "# Purpose: Runs when you click “Ask”.\n",
    "# What it does:\n",
    "# 1.Checks that a chain exists (i.e., you built the index first) and that a question is provided.\n",
    "# 2.Calls chain.invoke(question).\n",
    "# 3.Extracts:\n",
    "# result[\"answer\"] → the final text\n",
    "# result[\"sources\"] → which chunks were used\n",
    "# 4.Formats sources with show_sources(...).\n",
    "# 5.Returns both to the UI.\n",
    "# Why it matters: This is the live Q&A endpoint.\n",
    "\n",
    "def ui_ask(chain, question: str):\n",
    "    if chain is None:\n",
    "        return \"Please build the index first.\", \"\"\n",
    "    if not question or not question.strip():\n",
    "        return \"Please enter a question.\", \"\"\n",
    "    try:\n",
    "        result = chain.invoke(question.strip())\n",
    "        answer = result[\"answer\"]\n",
    "        docs = result[\"sources\"]\n",
    "        return answer.strip(), show_sources(docs)\n",
    "    except Exception as e:\n",
    "        return f\"❌ Error while answering: {e}\", \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd5341b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Build Gradio UI\n",
    "# =========================\n",
    "\n",
    "# 1.API Key textbox (optional if you set env var).\n",
    "# 2.Data source:\n",
    "# File input (type=filepath) to upload a different PDF\n",
    "# Checkbox to use default Amazon Bedrock - User Guide.pdf\n",
    "# 3.RAG settings: LLM, temperature, embeddings model, chunk size/overlap, retriever pool (top-N).\n",
    "# 4.Re-ranking settings: toggle ON/OFF, choose cross-encoder model, set rerank top-K.\n",
    "# 5.Build Index button → runs ui_build_index(...).\n",
    "# 6.Question textbox + Ask button → runs ui_ask(...).\n",
    "# 7.Answer (Markdown) + Sources (Textbox) display.\n",
    "# Why it matters: Learners can see how changing knobs (pool size, rerank top-K) affects quality.\n",
    "\n",
    "with gr.Blocks(title=\"RAG with Re-Ranking — PDF QA\") as demo:\n",
    "    gr.Markdown(\n",
    "        \"\"\"\n",
    "        # 🔎 RAG with Re-Ranking — PDF Question Answering\n",
    "        1) Provide your **OpenAI API key** (or set `OPENAI_API_KEY` in env).  \n",
    "        2) Use the default **Amazon Bedrock - User Guide.pdf** (place it in this folder) or upload a PDF.  \n",
    "        3) Choose retrieval pool size and re-ranking settings, then **Build Index**.  \n",
    "        4) Ask questions and compare re-rank ON vs OFF for relevance.\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "    with gr.Row():\n",
    "        api_key = gr.Textbox(\n",
    "            label=\"OpenAI API Key (optional if set in environment)\",\n",
    "            type=\"password\",\n",
    "            placeholder=\"sk-...\",\n",
    "        )\n",
    "\n",
    "    with gr.Accordion(\"Data source\", open=True):\n",
    "        with gr.Row():\n",
    "            pdf = gr.File(label=\"Upload PDF\", file_types=[\".pdf\"], type=\"filepath\")\n",
    "            use_default = gr.Checkbox(value=True, label=f\"Use default: {DEFAULT_LOCAL_PDF}\")\n",
    "\n",
    "    with gr.Accordion(\"RAG settings\", open=True):\n",
    "        with gr.Row():\n",
    "            model_name = gr.Dropdown(choices=[\"gpt-4o-mini\"], value=\"gpt-4o-mini\", label=\"LLM\")\n",
    "            temperature = gr.Slider(0.0, 1.0, value=0.0, step=0.1, label=\"Temperature\")\n",
    "        with gr.Row():\n",
    "            embed_model = gr.Dropdown(\n",
    "                choices=[\"text-embedding-3-small\", \"text-embedding-3-large\"],\n",
    "                value=\"text-embedding-3-small\",\n",
    "                label=\"Embeddings model\"\n",
    "            )\n",
    "            chunk_size = gr.Slider(200, 1200, value=500, step=50, label=\"Chunk size\")\n",
    "            chunk_overlap = gr.Slider(0, 400, value=80, step=10, label=\"Chunk overlap\")\n",
    "            pool_k = gr.Slider(5, 50, value=15, step=1, label=\"Retriever pool size (top-N)\")\n",
    "    with gr.Accordion(\"Re-ranking settings\", open=True):\n",
    "        with gr.Row():\n",
    "            use_rerank = gr.Checkbox(value=True, label=\"Enable Cross-Encoder Re-Ranking\")\n",
    "            rerank_model = gr.Dropdown(\n",
    "                choices=[\n",
    "                    \"cross-encoder/ms-marco-MiniLM-L-6-v2\",\n",
    "                    \"cross-encoder/ms-marco-MiniLM-L-12-v2\",\n",
    "                    \"cross-encoder/ms-marco-TinyBERT-L-2-v2\"\n",
    "                ],\n",
    "                value=\"cross-encoder/ms-marco-MiniLM-L-6-v2\",\n",
    "                label=\"Cross-Encoder model\"\n",
    "            )\n",
    "            rerank_top_k = gr.Slider(1, 15, value=5, step=1, label=\"Re-rank top-K\")\n",
    "\n",
    "    build_btn = gr.Button(\"🔧 Build Index\", variant=\"primary\")\n",
    "\n",
    "    index_summary = gr.Markdown(visible=False)\n",
    "    status = gr.Markdown(\"\")\n",
    "    state_chain = gr.State()\n",
    "\n",
    "    gr.Markdown(\"---\")\n",
    "    question = gr.Textbox(\n",
    "        label=\"Ask a question about the PDF\",\n",
    "        lines=2,\n",
    "        placeholder=\"e.g., What does Amazon Bedrock provide to developers?\"\n",
    "    )\n",
    "    ask_btn = gr.Button(\"💬 Ask\")\n",
    "    answer = gr.Markdown(label=\"Answer\")\n",
    "    sources = gr.Textbox(label=\"Sources (retrieved chunks/pages)\", lines=6)\n",
    "\n",
    "    build_btn.click(\n",
    "        ui_build_index,\n",
    "        inputs=[\n",
    "            api_key, pdf, use_default,\n",
    "            model_name, temperature, embed_model, chunk_size, chunk_overlap,\n",
    "            pool_k, use_rerank, rerank_model, rerank_top_k\n",
    "        ],\n",
    "        outputs=[state_chain, index_summary, status],\n",
    "        api_name=\"build_index\",\n",
    "    )\n",
    "\n",
    "    ask_btn.click(\n",
    "        ui_ask,\n",
    "        inputs=[state_chain, question],\n",
    "        outputs=[answer, sources],\n",
    "        api_name=\"ask\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82745655",
   "metadata": {},
   "outputs": [],
   "source": [
    "demo.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2149134",
   "metadata": {},
   "source": [
    "<u><b>End-to-end flow (quick recap)</u></b>\n",
    "\n",
    "1. Click Build Index → PDF is loaded, chunked, embedded, indexed; chain is prepared (with or without re-ranking).\n",
    "\n",
    "2. Type a question → The chain retrieves a broad pool → optionally re-ranks → sends top-K to the LLM → shows answer + sources.\n",
    "\n",
    "\n",
    "<u><b>What to tweak (for more learning/home work)</u></b>\n",
    "\n",
    "1.Retriever pool size (top-N): larger means better recall but more noise (and slower re-rank).\n",
    "\n",
    "2.Re-rank top-K: smaller K = tighter context (more precise), but risk missing something.\n",
    "\n",
    "3.Chunk size/overlap: bigger chunks carry more context but may dilute relevance; overlap helps preserve sentence continuity.\n",
    "\n",
    "4.Cross-encoder model: MiniLM-L-6-v2 is fast; larger ones can be more accurate but slower.\n",
    "\n",
    "5.Temperature: keep at 0.0 for factual answers; increase for more creative wording (not recommended for docs QA).\n",
    "\n",
    "\n",
    "<u><b>Common pitfalls (and fixes)</u></b>\n",
    "\n",
    "1.“Default PDF not found” → place Amazon Bedrock - User Guide.pdf next to the script or uncheck “Use default” and upload.\n",
    "\n",
    "2.“Missing OPENAI_API_KEY” → set env var or paste it into the UI field.\n",
    "\n",
    "3.Slow re-ranking → reduce pool_k or rerank_top_k, or choose a smaller cross-encoder.\n",
    "\n",
    "4.GPU not used → the cross-encoder auto-detects CUDA; if none, it uses CPU (slower but works).\n",
    "\n",
    "5.Large PDFs → consider persisting FAISS to disk and reusing the index; or pre-chunk offline."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
