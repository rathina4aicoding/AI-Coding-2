{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f023d9f",
   "metadata": {},
   "source": [
    "## Vanilla RAG (Basic Retrieval + Generation)\n",
    "\n",
    "How it works:\n",
    "\n",
    "User asks a question.\n",
    "\n",
    "System searches a knowledge base (like a PDF, wiki, or DB).\n",
    "\n",
    "Retrieved documents are passed into the LLM prompt.\n",
    "\n",
    "LLM generates an answer using both retrieved info + its own knowledge.\n",
    "\n",
    "Example Use Case:\n",
    "A bank‚Äôs customer support chatbot retrieves the latest interest rates from a database and uses the LLM to explain them in simple words to the customer.\n",
    "\n",
    "User Query ‚Üí Retriever (Search DB) ‚Üí Relevant Docs ‚Üí LLM ‚Üí Final Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c237f0",
   "metadata": {},
   "source": [
    "## How the whole thing runs?\n",
    "\n",
    "Build Index\n",
    "‚Üí Resolve PDF path ‚Üí Load PDF pages ‚Üí Split into chunks ‚Üí Embed ‚Üí FAISS index ‚Üí Make retriever ‚Üí Make RAG chain.\n",
    "\n",
    "Ask\n",
    "‚Üí Take your question ‚Üí Retrieve top-k chunks ‚Üí Format context ‚Üí LLM answers only from those chunks ‚Üí Show answer + sources.\n",
    "\n",
    "\n",
    "## Walkthrough of Each Routine\n",
    "\n",
    "build_index() ‚Üí Reads PDF ‚Üí Splits into chunks ‚Üí Embeds ‚Üí Builds FAISS.\n",
    "\n",
    "make_chain() ‚Üí Connects retriever + LLM ‚Üí RAG pipeline.\n",
    "\n",
    "ui_build_index() ‚Üí Trigger to build the index (once).\n",
    "\n",
    "ui_ask_question() ‚Üí Trigger to ask a question ‚Üí Retrieves + Answers.\n",
    "\n",
    "Gradio UI ‚Üí Provides the friendly interface with buttons, boxes, answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3166794",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tempfile\n",
    "from typing import List, Tuple, Dict, Any\n",
    "\n",
    "import gradio as gr\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# LangChain pieces\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema import Document\n",
    "from langchain.schema.runnable import RunnablePassthrough, RunnableMap\n",
    "from langchain_core.output_parsers import StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e958a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables in a file called .env\n",
    "\n",
    "load_dotenv(override=True)\n",
    "os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY', 'your-key-if-not-using-env')\n",
    "#openai = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a1b8b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Prompt (grounded, concise). This is a set of rules for the LLM to follow.Only use the provided context (chunks from the PDF).\n",
    "#A message template the LLM will see:\n",
    "#It includes the Question and the Context (retrieved chunks).\n",
    "#The model is instructed to ground the answer in that context.\n",
    "#These keep the LLM factual and reduce hallucinations. (Defined near the top under ‚ÄúPrompt (grounded, concise)‚Äù.\n",
    "# -----------------------------\n",
    "SYSTEM_PROMPT = \"\"\"You are a factual assistant. Answer ONLY using the provided context.\n",
    "If the answer is not in the context, say you don't know.\n",
    "Be concise and include short inline citations like [source] when helpful.\"\"\"\n",
    "\n",
    "PROMPT = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", SYSTEM_PROMPT),\n",
    "        (\"human\",\n",
    "         \"Question:\\n{question}\\n\\n\"\n",
    "         \"Context:\\n{context}\\n\\n\"\n",
    "         \"Answer with facts grounded in the context.\")\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2928504",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Purpose: Takes a list of retrieved chunks and turns them into a single readable context string with simple citations.\n",
    "# How it works:\n",
    "# Loops each Document d.\n",
    "# Builds labels like [Amazon Bedrock - User Guide.pdf (page 3)].\n",
    "# Concatenates their page_content into one string the LLM can read.\n",
    "# Why it matters: LLMs need a single text block of context; this function creates it and preserves where each chunk came from.      \n",
    "  \n",
    "  \n",
    "def format_docs(docs: List[Document]) -> str:\n",
    "    \"\"\"Join retrieved docs into a single context string with lightweight citations.\"\"\"\n",
    "    lines = []\n",
    "    for i, d in enumerate(docs, start=1):\n",
    "        src = d.metadata.get(\"source\", f\"doc_{i}\")\n",
    "        page = d.metadata.get(\"page\", None)\n",
    "        page_str = f\" (page {page+1})\" if isinstance(page, int) else \"\"\n",
    "        lines.append(f\"[{src}{page_str}] {d.page_content}\")\n",
    "    return \"\\n\\n\".join(lines)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a8f6b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads the PDF with PyPDFLoader (each page becomes a doc).\n",
    "# Splits into chunks (‚âà500 characters, with overlap of 80 for context continuity).\n",
    "# Embeds chunks into vectors using OpenAI‚Äôs embedding model.\n",
    "# Stores vectors in a FAISS index (so we can search them later).\n",
    "# Returns a retriever (a helper that fetches top-k chunks given a query).\n",
    "# This is the first step in the RAG pipeline. (\"knowledge base creation‚Äù)\n",
    "\n",
    "def build_index(\n",
    "    pdf_path: str,\n",
    "    api_key: str,\n",
    "    embed_model: str = \"text-embedding-3-small\",\n",
    "    chunk_size: int = 500,\n",
    "    chunk_overlap: int = 80,\n",
    "    k: int = 4,\n",
    ") -> Tuple[Any, Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Loads a PDF, splits into chunks, creates embeddings, and returns a retriever.\n",
    "    \"\"\"\n",
    "    if api_key:\n",
    "        os.environ[\"OPENAI_API_KEY\"] = api_key\n",
    "    if not os.getenv(\"OPENAI_API_KEY\"):\n",
    "        raise RuntimeError(\"Missing OPENAI_API_KEY. Provide in the textbox or environment.\")\n",
    "\n",
    "    # Load PDF (each page becomes a Document with page metadata)\n",
    "    loader = PyPDFLoader(pdf_path)\n",
    "    raw_docs = loader.load()  # List[Document]\n",
    "    # Add 'source' to each page so citations look nice\n",
    "    for d in raw_docs:\n",
    "        d.metadata[\"source\"] = os.path.basename(pdf_path)\n",
    "\n",
    "    # Split into chunks\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size, chunk_overlap=chunk_overlap, add_start_index=True\n",
    "    )\n",
    "    chunks: List[Document] = splitter.split_documents(raw_docs)\n",
    "\n",
    "    # Vector index\n",
    "    embeddings = OpenAIEmbeddings(model=embed_model)\n",
    "    vectorstore = FAISS.from_documents(chunks, embedding=embeddings)\n",
    "    retriever = vectorstore.as_retriever(search_kwargs={\"k\": k})\n",
    "\n",
    "    stats = {\n",
    "        \"pages\": len(raw_docs),\n",
    "        \"chunks\": len(chunks),\n",
    "        \"pdf_name\": os.path.basename(pdf_path),\n",
    "        \"k\": k,\n",
    "        \"chunk_size\": chunk_size,\n",
    "        \"chunk_overlap\": chunk_overlap,\n",
    "        \"embed_model\": embed_model,\n",
    "    }\n",
    "    return retriever, stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50701102",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Builds the chain (combines retriever + LLM).\n",
    "# This is the second step in the RAG pipeline. (\"chain creation\")\n",
    "# This chain is used to answer questions.\n",
    "# It takes the question and the retrieved chunks, formats them, and passes them to the LLM.\n",
    "# The LLM then generates an answer using both the retrieved info + its own knowledge.\n",
    "# This is the third step in the RAG pipeline. (\"chain creation\")\n",
    "\n",
    "def make_chain(retriever, model_name: str = \"gpt-4o-mini\", temperature: float = 0.0):\n",
    "    llm = ChatOpenAI(model=model_name, temperature=temperature)\n",
    "    chain = (\n",
    "        RunnableMap(\n",
    "            {\"question\": RunnablePassthrough(), \"docs\": retriever}\n",
    "        )\n",
    "        | RunnableMap(\n",
    "            {\n",
    "                \"question\": lambda x: x[\"question\"],\n",
    "                \"context\": lambda x: format_docs(x[\"docs\"]),\n",
    "                \"docs\": lambda x: x[\"docs\"],\n",
    "            }\n",
    "        )\n",
    "        | RunnableMap(\n",
    "            {\n",
    "                \"answer\": PROMPT | llm | StrOutputParser(),\n",
    "                \"docs\": lambda x: x[\"docs\"],\n",
    "            }\n",
    "        )\n",
    "    )\n",
    "    return chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6daf5717",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Purpose: Pretty-prints which chunks/pages were used in the answer.\n",
    "# How it works:\n",
    "# For each Document, prints source (filename), page, and start_index (the character offset set by the splitter).\n",
    "# Returns a readable bullet list.\n",
    "# Why it matters: Learners see transparency‚Äîexactly which pages powered the answer.\n",
    "\n",
    "def describe_sources(docs: List[Document]) -> str:\n",
    "    \"\"\"Pretty-print the sources used.\"\"\"\n",
    "    if not docs:\n",
    "        return \"No sources retrieved.\"\n",
    "    lines = []\n",
    "    for d in docs:\n",
    "        src = d.metadata.get(\"source\", \"unknown\")\n",
    "        page = d.metadata.get(\"page\", None)\n",
    "        start_idx = d.metadata.get(\"start_index\", \"?\")\n",
    "        page_str = f\"page {page+1}\" if isinstance(page, int) else \"page ?\"\n",
    "        lines.append(f\"- {src} ({page_str}), start_char={start_idx}\")\n",
    "    return \"\\n\".join(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6512466",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Purpose: If Gradio returns raw bytes (instead of a file path), this makes a temporary .pdf file and returns its path.\n",
    "# How it works:\n",
    "# Creates a temp folder, writes the bytes to uploaded.pdf.\n",
    "# Returns that file path.\n",
    "# Why it matters: Later steps (PDF loader) need a filesystem path, not raw bytes\n",
    "\n",
    "def _bytes_to_temp_pdf(file_bytes: bytes, suggested_name: str = \"uploaded.pdf\") -> str:\n",
    "    \"\"\"Write bytes to a temp .pdf and return the path.\"\"\"\n",
    "    tmp_dir = tempfile.mkdtemp()\n",
    "    pdf_path = os.path.join(tmp_dir, suggested_name)\n",
    "    with open(pdf_path, \"wb\") as f:\n",
    "        f.write(file_bytes)\n",
    "    return pdf_path\n",
    "\n",
    "# Purpose: Robustly figure out the actual PDF path to load.\n",
    "# How it works:\n",
    "# If use_default=True, ensure Amazon Bedrock - User Guide.pdf exists and return it.\n",
    "# Else, if user uploaded:\n",
    "# If it‚Äôs a string path (Gradio type=\"filepath\"), verify and return it.\n",
    "# If it‚Äôs bytes, call _bytes_to_temp_pdf(...) and return that path.\n",
    "# Otherwise, raise a helpful error.\n",
    "# Why it matters: No matter how the file arrives, the rest of the code always gets a valid path.\n",
    "\n",
    "def _resolve_pdf_path(pdf_input, use_default: bool) -> str:\n",
    "    \"\"\"\n",
    "    Accepts:\n",
    "      - use_default=True: use local default file\n",
    "      - pdf_input: either a filepath (str) or bytes (if someone changes the File 'type')\n",
    "    Returns a filesystem path to a readable PDF.\n",
    "    \"\"\"\n",
    "    if use_default:\n",
    "        default_path = \"Amazon Bedrock - User Guide.pdf\"\n",
    "        if not os.path.exists(default_path):\n",
    "            raise FileNotFoundError(\"Default PDF not found in project folder.\")\n",
    "        return default_path\n",
    "\n",
    "    if pdf_input is None:\n",
    "        raise ValueError(\"Please upload a PDF or check 'Use default PDF'.\")\n",
    "\n",
    "    # If File(type=\"filepath\"), Gradio gives a string path\n",
    "    if isinstance(pdf_input, str):\n",
    "        if not os.path.exists(pdf_input):\n",
    "            raise FileNotFoundError(f\"Uploaded path not found: {pdf_input}\")\n",
    "        return pdf_input\n",
    "\n",
    "    # If someone kept type=\"binary\", Gradio gives bytes\n",
    "    if isinstance(pdf_input, (bytes, bytearray)):\n",
    "        return _bytes_to_temp_pdf(pdf_input)\n",
    "\n",
    "    # Fallback (rare)\n",
    "    raise TypeError(f\"Unsupported file input type: {type(pdf_input)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f89a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Gradio Callbacks\n",
    "## ui_build_index()\n",
    "# Triggered when you click ‚ÄúBuild Index‚Äù in the UI.\n",
    "# Uses either the default \"Amazon Bedrock - User Guide.pdf\" or your uploaded PDF.\n",
    "# Calls build_index() ‚Üí creates retriever.\n",
    "# Calls make_chain() ‚Üí creates RAG pipeline.\n",
    "# Stores the pipeline in a hidden Gradio State so we can reuse it.\n",
    "# Returns a summary (pages, chunks, etc.) to show the user.\n",
    "#\n",
    "## ui_ask_question()\n",
    "# Triggered when you click ‚ÄúAsk‚Äù.\n",
    "# Sends your question to the RAG chain.\n",
    "# Gets back an answer + retrieved docs.\n",
    "# Returns both to the UI.\n",
    "# -----------------------------\n",
    "def ui_build_index(api_key: str, pdf_file, use_default: bool):\n",
    "    \"\"\"\n",
    "    Builds an index from either the uploaded PDF or default local file.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        pdf_path = _resolve_pdf_path(pdf_file, use_default)\n",
    "        retriever, stats = build_index(pdf_path, api_key=api_key)\n",
    "        # Build a default chain so we can reuse it across questions\n",
    "        chain = make_chain(retriever)\n",
    "        summary = (\n",
    "            f\"‚úÖ Index ready for **{stats['pdf_name']}**\\n\"\n",
    "            f\"- Pages: {stats['pages']}\\n\"\n",
    "            f\"- Chunks: {stats['chunks']}\\n\"\n",
    "            f\"- Top-K: {stats['k']}\\n\"\n",
    "            f\"- Chunk size/overlap: {stats['chunk_size']}/{stats['chunk_overlap']}\\n\"\n",
    "            f\"- Embeddings: {stats['embed_model']}\"\n",
    "        )\n",
    "        return chain, gr.update(value=summary, visible=True), \"Index built successfully.\"\n",
    "    except Exception as e:\n",
    "        return None, gr.update(value=\"\", visible=True), f\"‚ùå Error: {e}\"\n",
    "\n",
    "def ui_ask_question(chain, question: str):\n",
    "    if chain is None:\n",
    "        return \"Please build the index first.\", \"\"\n",
    "    if not question or not question.strip():\n",
    "        return \"Please enter a question.\", \"\"\n",
    "    try:\n",
    "        result = chain.invoke(question.strip())\n",
    "        answer = result[\"answer\"]\n",
    "        docs = result[\"docs\"]\n",
    "        sources = describe_sources(docs)\n",
    "        return answer.strip(), sources\n",
    "    except Exception as e:\n",
    "        return f\"‚ùå Error while answering: {e}\", \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "066b40dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------\n",
    "# Build Gradio App\n",
    "# Layout:\n",
    "# Input box for API key.\n",
    "# Upload PDF or checkbox for default.\n",
    "# Button to Build Index.\n",
    "# Question box + Ask button.\n",
    "# Answer + Sources display area.\n",
    "# Event handlers:\n",
    "# build_btn.click(...) runs ui_build_index().\n",
    "# ask_btn.click(...) runs ui_ask_question().\n",
    "# This is the interactive web app part.\n",
    "# -----------------------------------------\n",
    "with gr.Blocks(title=\"Vanilla RAG ‚Äî PDF QA\") as demo:\n",
    "    gr.Markdown(\n",
    "        \"\"\"\n",
    "        # üîé Vanilla RAG ‚Äî PDF Question Answering\n",
    "        1) Provide your **OpenAI API key** (or set `OPENAI_API_KEY` in env).  \n",
    "        2) Upload a **PDF** *or* use the default `Amazon Bedrock - User Guide.pdf` in this folder.  \n",
    "        3) Click **Build Index** ‚Üí Ask questions grounded in the document.\n",
    "        \"\"\"\n",
    "    )\n",
    "    with gr.Row():\n",
    "        api_key = gr.Textbox(\n",
    "            label=\"OpenAI API Key (optional if set in environment)\",\n",
    "            type=\"password\",\n",
    "            placeholder=\"sk-...\",\n",
    "        )\n",
    "    with gr.Row():\n",
    "        # IMPORTANT: use filepath (not binary) to avoid bytes/no .name errors\n",
    "        pdf = gr.File(label=\"Upload PDF\", file_types=[\".pdf\"], type=\"filepath\")\n",
    "        use_default = gr.Checkbox(value=True, label=\"Use default: Amazon Bedrock - User Guide.pdf\")\n",
    "    build_btn = gr.Button(\"üîß Build Index\", variant=\"primary\")\n",
    "\n",
    "    index_summary = gr.Markdown(visible=False)\n",
    "    status = gr.Markdown(\"\")\n",
    "\n",
    "    state_chain = gr.State()  # holds the compiled chain\n",
    "\n",
    "    gr.Markdown(\"---\")\n",
    "    question = gr.Textbox(label=\"Ask a question about the PDF\", lines=2, placeholder=\"e.g., What is Amazon Bedrock and what does it provide?\")\n",
    "    ask_btn = gr.Button(\"üí¨ Ask\")\n",
    "    answer = gr.Markdown(label=\"Answer\")\n",
    "    sources = gr.Textbox(label=\"Sources (retrieved chunks/pages)\", lines=6)\n",
    "\n",
    "    build_btn.click(\n",
    "        ui_build_index,\n",
    "        inputs=[api_key, pdf, use_default],\n",
    "        outputs=[state_chain, index_summary, status],\n",
    "        api_name=\"build_index\",\n",
    "    )\n",
    "\n",
    "    ask_btn.click(\n",
    "        ui_ask_question,\n",
    "        inputs=[state_chain, question],\n",
    "        outputs=[answer, sources],\n",
    "        api_name=\"ask\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a3751d",
   "metadata": {},
   "outputs": [],
   "source": [
    "demo.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37965559",
   "metadata": {},
   "source": [
    "## Quick tips for learners\n",
    "\n",
    "1. Chunk size / overlap: tweak for better recall vs. noise.\n",
    "\n",
    "2. Top-K (k): bigger k gives more context but risks irrelevant text; smaller k is stricter.\n",
    "\n",
    "3. Temperature=0: best for factual answers.\n",
    "\n",
    "4. Where‚Äôs the data coming from? Always check the Sources panel."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
