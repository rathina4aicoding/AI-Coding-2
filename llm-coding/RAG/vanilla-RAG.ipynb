{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f023d9f",
   "metadata": {},
   "source": [
    "## Vanilla RAG (Basic Retrieval + Generation)\n",
    "\n",
    "How it works:\n",
    "\n",
    "User asks a question.\n",
    "\n",
    "System searches a knowledge base (like a PDF, wiki, or DB).\n",
    "\n",
    "Retrieved documents are passed into the LLM prompt.\n",
    "\n",
    "LLM generates an answer using both retrieved info + its own knowledge.\n",
    "\n",
    "Example Use Case:\n",
    "A bank’s customer support chatbot retrieves the latest interest rates from a database and uses the LLM to explain them in simple words to the customer.\n",
    "\n",
    "User Query → Retriever (Search DB) → Relevant Docs → LLM → Final Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c237f0",
   "metadata": {},
   "source": [
    "## How the whole thing runs?\n",
    "\n",
    "Build Index\n",
    "→ Resolve PDF path → Load PDF pages → Split into chunks → Embed → FAISS index → Make retriever → Make RAG chain.\n",
    "\n",
    "Ask\n",
    "→ Take your question → Retrieve top-k chunks → Format context → LLM answers only from those chunks → Show answer + sources.\n",
    "\n",
    "\n",
    "## Walkthrough of Each Routine\n",
    "\n",
    "build_index() → Reads PDF → Splits into chunks → Embeds → Builds FAISS.\n",
    "\n",
    "make_chain() → Connects retriever + LLM → RAG pipeline.\n",
    "\n",
    "ui_build_index() → Trigger to build the index (once).\n",
    "\n",
    "ui_ask_question() → Trigger to ask a question → Retrieves + Answers.\n",
    "\n",
    "Gradio UI → Provides the friendly interface with buttons, boxes, answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3166794",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tempfile\n",
    "from typing import List, Tuple, Dict, Any\n",
    "\n",
    "import gradio as gr\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# LangChain pieces\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema import Document\n",
    "from langchain.schema.runnable import RunnablePassthrough, RunnableMap\n",
    "from langchain_core.output_parsers import StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e958a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables in a file called .env\n",
    "\n",
    "load_dotenv(override=True)\n",
    "os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY', 'your-key-if-not-using-env')\n",
    "#openai = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a1b8b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Prompt (grounded, concise). This is a set of rules for the LLM to follow.Only use the provided context (chunks from the PDF).\n",
    "#A message template the LLM will see:\n",
    "#It includes the Question and the Context (retrieved chunks).\n",
    "#The model is instructed to ground the answer in that context.\n",
    "#These keep the LLM factual and reduce hallucinations. (Defined near the top under “Prompt (grounded, concise)”.\n",
    "# -----------------------------\n",
    "SYSTEM_PROMPT = \"\"\"You are a factual assistant. Answer ONLY using the provided context.\n",
    "If the answer is not in the context, say you don't know.\n",
    "Be concise and include short inline citations like [source] when helpful.\"\"\"\n",
    "\n",
    "PROMPT = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", SYSTEM_PROMPT),\n",
    "        (\"human\",\n",
    "         \"Question:\\n{question}\\n\\n\"\n",
    "         \"Context:\\n{context}\\n\\n\"\n",
    "         \"Answer with facts grounded in the context.\")\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2928504",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Purpose: Takes a list of retrieved chunks and turns them into a single readable context string with simple citations.\n",
    "# How it works:\n",
    "# Loops each Document d.\n",
    "# Builds labels like [Amazon Bedrock - User Guide.pdf (page 3)].\n",
    "# Concatenates their page_content into one string the LLM can read.\n",
    "# Why it matters: LLMs need a single text block of context; this function creates it and preserves where each chunk came from.      \n",
    "  \n",
    "  \n",
    "def format_docs(docs: List[Document]) -> str:\n",
    "    \"\"\"Join retrieved docs into a single context string with lightweight citations.\"\"\"\n",
    "    lines = []\n",
    "    for i, d in enumerate(docs, start=1):\n",
    "        src = d.metadata.get(\"source\", f\"doc_{i}\")\n",
    "        page = d.metadata.get(\"page\", None)\n",
    "        page_str = f\" (page {page+1})\" if isinstance(page, int) else \"\"\n",
    "        lines.append(f\"[{src}{page_str}] {d.page_content}\")\n",
    "    return \"\\n\\n\".join(lines)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a8f6b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads the PDF with PyPDFLoader (each page becomes a doc).\n",
    "# Splits into chunks (≈500 characters, with overlap of 80 for context continuity).\n",
    "# Embeds chunks into vectors using OpenAI’s embedding model.\n",
    "# Stores vectors in a FAISS index (so we can search them later).\n",
    "# Returns a retriever (a helper that fetches top-k chunks given a query).\n",
    "# This is the first step in the RAG pipeline. (\"knowledge base creation”)\n",
    "\n",
    "def build_index(\n",
    "    pdf_path: str,\n",
    "    api_key: str,\n",
    "    embed_model: str = \"text-embedding-3-small\",\n",
    "    chunk_size: int = 500,\n",
    "    chunk_overlap: int = 80,\n",
    "    k: int = 4,\n",
    ") -> Tuple[Any, Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Loads a PDF, splits into chunks, creates embeddings, and returns a retriever.\n",
    "    \"\"\"\n",
    "    if api_key:\n",
    "        os.environ[\"OPENAI_API_KEY\"] = api_key\n",
    "    if not os.getenv(\"OPENAI_API_KEY\"):\n",
    "        raise RuntimeError(\"Missing OPENAI_API_KEY. Provide in the textbox or environment.\")\n",
    "\n",
    "    # Load PDF (each page becomes a Document with page metadata)\n",
    "    loader = PyPDFLoader(pdf_path)\n",
    "    raw_docs = loader.load()  # List[Document]\n",
    "    # Add 'source' to each page so citations look nice\n",
    "    for d in raw_docs:\n",
    "        d.metadata[\"source\"] = os.path.basename(pdf_path)\n",
    "\n",
    "    # Split into chunks\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size, chunk_overlap=chunk_overlap, add_start_index=True\n",
    "    )\n",
    "    chunks: List[Document] = splitter.split_documents(raw_docs)\n",
    "\n",
    "    # Vector index\n",
    "    embeddings = OpenAIEmbeddings(model=embed_model)\n",
    "    vectorstore = FAISS.from_documents(chunks, embedding=embeddings)\n",
    "    retriever = vectorstore.as_retriever(search_kwargs={\"k\": k})\n",
    "\n",
    "    stats = {\n",
    "        \"pages\": len(raw_docs),\n",
    "        \"chunks\": len(chunks),\n",
    "        \"pdf_name\": os.path.basename(pdf_path),\n",
    "        \"k\": k,\n",
    "        \"chunk_size\": chunk_size,\n",
    "        \"chunk_overlap\": chunk_overlap,\n",
    "        \"embed_model\": embed_model,\n",
    "    }\n",
    "    return retriever, stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50701102",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Builds the chain (combines retriever + LLM).\n",
    "# This is the second step in the RAG pipeline. (\"chain creation\")\n",
    "# This chain is used to answer questions.\n",
    "# It takes the question and the retrieved chunks, formats them, and passes them to the LLM.\n",
    "# The LLM then generates an answer using both the retrieved info + its own knowledge.\n",
    "# This is the third step in the RAG pipeline. (\"chain creation\")\n",
    "\n",
    "def make_chain(retriever, model_name: str = \"gpt-4o-mini\", temperature: float = 0.0):\n",
    "    llm = ChatOpenAI(model=model_name, temperature=temperature)\n",
    "    chain = (\n",
    "        RunnableMap(\n",
    "            {\"question\": RunnablePassthrough(), \"docs\": retriever}\n",
    "        )\n",
    "        | RunnableMap(\n",
    "            {\n",
    "                \"question\": lambda x: x[\"question\"],\n",
    "                \"context\": lambda x: format_docs(x[\"docs\"]),\n",
    "                \"docs\": lambda x: x[\"docs\"],\n",
    "            }\n",
    "        )\n",
    "        | RunnableMap(\n",
    "            {\n",
    "                \"answer\": PROMPT | llm | StrOutputParser(),\n",
    "                \"docs\": lambda x: x[\"docs\"],\n",
    "            }\n",
    "        )\n",
    "    )\n",
    "    return chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6daf5717",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Purpose: Pretty-prints which chunks/pages were used in the answer.\n",
    "# How it works:\n",
    "# For each Document, prints source (filename), page, and start_index (the character offset set by the splitter).\n",
    "# Returns a readable bullet list.\n",
    "# Why it matters: Learners see transparency—exactly which pages powered the answer.\n",
    "\n",
    "def describe_sources(docs: List[Document]) -> str:\n",
    "    \"\"\"Pretty-print the sources used.\"\"\"\n",
    "    if not docs:\n",
    "        return \"No sources retrieved.\"\n",
    "    lines = []\n",
    "    for d in docs:\n",
    "        src = d.metadata.get(\"source\", \"unknown\")\n",
    "        page = d.metadata.get(\"page\", None)\n",
    "        start_idx = d.metadata.get(\"start_index\", \"?\")\n",
    "        page_str = f\"page {page+1}\" if isinstance(page, int) else \"page ?\"\n",
    "        lines.append(f\"- {src} ({page_str}), start_char={start_idx}\")\n",
    "    return \"\\n\".join(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6512466",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Purpose: If Gradio returns raw bytes (instead of a file path), this makes a temporary .pdf file and returns its path.\n",
    "# How it works:\n",
    "# Creates a temp folder, writes the bytes to uploaded.pdf.\n",
    "# Returns that file path.\n",
    "# Why it matters: Later steps (PDF loader) need a filesystem path, not raw bytes\n",
    "\n",
    "def _bytes_to_temp_pdf(file_bytes: bytes, suggested_name: str = \"uploaded.pdf\") -> str:\n",
    "    \"\"\"Write bytes to a temp .pdf and return the path.\"\"\"\n",
    "    tmp_dir = tempfile.mkdtemp()\n",
    "    pdf_path = os.path.join(tmp_dir, suggested_name)\n",
    "    with open(pdf_path, \"wb\") as f:\n",
    "        f.write(file_bytes)\n",
    "    return pdf_path\n",
    "\n",
    "# Purpose: Robustly figure out the actual PDF path to load.\n",
    "# How it works:\n",
    "# If use_default=True, ensure Amazon Bedrock - User Guide.pdf exists and return it.\n",
    "# Else, if user uploaded:\n",
    "# If it’s a string path (Gradio type=\"filepath\"), verify and return it.\n",
    "# If it’s bytes, call _bytes_to_temp_pdf(...) and return that path.\n",
    "# Otherwise, raise a helpful error.\n",
    "# Why it matters: No matter how the file arrives, the rest of the code always gets a valid path.\n",
    "\n",
    "def _resolve_pdf_path(pdf_input, use_default: bool) -> str:\n",
    "    \"\"\"\n",
    "    Accepts:\n",
    "      - use_default=True: use local default file\n",
    "      - pdf_input: either a filepath (str) or bytes (if someone changes the File 'type')\n",
    "    Returns a filesystem path to a readable PDF.\n",
    "    \"\"\"\n",
    "    if use_default:\n",
    "        default_path = \"Amazon Bedrock - User Guide.pdf\"\n",
    "        if not os.path.exists(default_path):\n",
    "            raise FileNotFoundError(\"Default PDF not found in project folder.\")\n",
    "        return default_path\n",
    "\n",
    "    if pdf_input is None:\n",
    "        raise ValueError(\"Please upload a PDF or check 'Use default PDF'.\")\n",
    "\n",
    "    # If File(type=\"filepath\"), Gradio gives a string path\n",
    "    if isinstance(pdf_input, str):\n",
    "        if not os.path.exists(pdf_input):\n",
    "            raise FileNotFoundError(f\"Uploaded path not found: {pdf_input}\")\n",
    "        return pdf_input\n",
    "\n",
    "    # If someone kept type=\"binary\", Gradio gives bytes\n",
    "    if isinstance(pdf_input, (bytes, bytearray)):\n",
    "        return _bytes_to_temp_pdf(pdf_input)\n",
    "\n",
    "    # Fallback (rare)\n",
    "    raise TypeError(f\"Unsupported file input type: {type(pdf_input)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f89a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Gradio Callbacks\n",
    "## ui_build_index()\n",
    "# Triggered when you click “Build Index” in the UI.\n",
    "# Uses either the default \"Amazon Bedrock - User Guide.pdf\" or your uploaded PDF.\n",
    "# Calls build_index() → creates retriever.\n",
    "# Calls make_chain() → creates RAG pipeline.\n",
    "# Stores the pipeline in a hidden Gradio State so we can reuse it.\n",
    "# Returns a summary (pages, chunks, etc.) to show the user.\n",
    "#\n",
    "## ui_ask_question()\n",
    "# Triggered when you click “Ask”.\n",
    "# Sends your question to the RAG chain.\n",
    "# Gets back an answer + retrieved docs.\n",
    "# Returns both to the UI.\n",
    "# -----------------------------\n",
    "def ui_build_index(api_key: str, pdf_file, use_default: bool):\n",
    "    \"\"\"\n",
    "    Builds an index from either the uploaded PDF or default local file.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        pdf_path = _resolve_pdf_path(pdf_file, use_default)\n",
    "        retriever, stats = build_index(pdf_path, api_key=api_key)\n",
    "        # Build a default chain so we can reuse it across questions\n",
    "        chain = make_chain(retriever)\n",
    "        summary = (\n",
    "            f\"✅ Index ready for **{stats['pdf_name']}**\\n\"\n",
    "            f\"- Pages: {stats['pages']}\\n\"\n",
    "            f\"- Chunks: {stats['chunks']}\\n\"\n",
    "            f\"- Top-K: {stats['k']}\\n\"\n",
    "            f\"- Chunk size/overlap: {stats['chunk_size']}/{stats['chunk_overlap']}\\n\"\n",
    "            f\"- Embeddings: {stats['embed_model']}\"\n",
    "        )\n",
    "        return chain, gr.update(value=summary, visible=True), \"Index built successfully.\"\n",
    "    except Exception as e:\n",
    "        return None, gr.update(value=\"\", visible=True), f\"❌ Error: {e}\"\n",
    "\n",
    "def ui_ask_question(chain, question: str):\n",
    "    if chain is None:\n",
    "        return \"Please build the index first.\", \"\"\n",
    "    if not question or not question.strip():\n",
    "        return \"Please enter a question.\", \"\"\n",
    "    try:\n",
    "        result = chain.invoke(question.strip())\n",
    "        answer = result[\"answer\"]\n",
    "        docs = result[\"docs\"]\n",
    "        sources = describe_sources(docs)\n",
    "        return answer.strip(), sources\n",
    "    except Exception as e:\n",
    "        return f\"❌ Error while answering: {e}\", \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "066b40dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------\n",
    "# Build Gradio App\n",
    "# Layout:\n",
    "# Input box for API key.\n",
    "# Upload PDF or checkbox for default.\n",
    "# Button to Build Index.\n",
    "# Question box + Ask button.\n",
    "# Answer + Sources display area.\n",
    "# Event handlers:\n",
    "# build_btn.click(...) runs ui_build_index().\n",
    "# ask_btn.click(...) runs ui_ask_question().\n",
    "# This is the interactive web app part.\n",
    "# -----------------------------------------\n",
    "with gr.Blocks(title=\"Vanilla RAG — PDF QA\") as demo:\n",
    "    gr.Markdown(\n",
    "        \"\"\"\n",
    "        # 🔎 Vanilla RAG — PDF Question Answering\n",
    "        1) Provide your **OpenAI API key** (or set `OPENAI_API_KEY` in env).  \n",
    "        2) Upload a **PDF** *or* use the default `Amazon Bedrock - User Guide.pdf` in this folder.  \n",
    "        3) Click **Build Index** → Ask questions grounded in the document.\n",
    "        \"\"\"\n",
    "    )\n",
    "    with gr.Row():\n",
    "        api_key = gr.Textbox(\n",
    "            label=\"OpenAI API Key (optional if set in environment)\",\n",
    "            type=\"password\",\n",
    "            placeholder=\"sk-...\",\n",
    "        )\n",
    "    with gr.Row():\n",
    "        # IMPORTANT: use filepath (not binary) to avoid bytes/no .name errors\n",
    "        pdf = gr.File(label=\"Upload PDF\", file_types=[\".pdf\"], type=\"filepath\")\n",
    "        use_default = gr.Checkbox(value=True, label=\"Use default: Amazon Bedrock - User Guide.pdf\")\n",
    "    build_btn = gr.Button(\"🔧 Build Index\", variant=\"primary\")\n",
    "\n",
    "    index_summary = gr.Markdown(visible=False)\n",
    "    status = gr.Markdown(\"\")\n",
    "\n",
    "    state_chain = gr.State()  # holds the compiled chain\n",
    "\n",
    "    gr.Markdown(\"---\")\n",
    "    question = gr.Textbox(label=\"Ask a question about the PDF\", lines=2, placeholder=\"e.g., What is Amazon Bedrock and what does it provide?\")\n",
    "    ask_btn = gr.Button(\"💬 Ask\")\n",
    "    answer = gr.Markdown(label=\"Answer\")\n",
    "    sources = gr.Textbox(label=\"Sources (retrieved chunks/pages)\", lines=6)\n",
    "\n",
    "    build_btn.click(\n",
    "        ui_build_index,\n",
    "        inputs=[api_key, pdf, use_default],\n",
    "        outputs=[state_chain, index_summary, status],\n",
    "        api_name=\"build_index\",\n",
    "    )\n",
    "\n",
    "    ask_btn.click(\n",
    "        ui_ask_question,\n",
    "        inputs=[state_chain, question],\n",
    "        outputs=[answer, sources],\n",
    "        api_name=\"ask\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a3751d",
   "metadata": {},
   "outputs": [],
   "source": [
    "demo.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37965559",
   "metadata": {},
   "source": [
    "## Quick tips for learners\n",
    "\n",
    "1. Chunk size / overlap: tweak for better recall vs. noise.\n",
    "\n",
    "2. Top-K (k): bigger k gives more context but risks irrelevant text; smaller k is stricter.\n",
    "\n",
    "3. Temperature=0: best for factual answers.\n",
    "\n",
    "4. Where’s the data coming from? Always check the Sources panel."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
