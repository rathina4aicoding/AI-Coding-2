{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b794be4d",
   "metadata": {},
   "source": [
    "# Why run open-source LLM calls on Google Colab?\n",
    "\n",
    "### Access to GPU without owning one\n",
    "\n",
    "1. Many open-source LLMs (like LLaMA, Mistral, Qwen) need lots of VRAM to run efficiently.\n",
    "\n",
    "2. Colab’s free tier gives you a small GPU (usually T4), and Pro/Pro+ tiers give you faster GPUs (P100, V100, A100) for a few dollars per month.\n",
    "\n",
    "3. Saves you from buying expensive hardware.\n",
    "\n",
    "\n",
    "### Pre-installed ML ecosystem\n",
    "\n",
    "1. Comes with PyTorch, Hugging Face Transformers, CUDA drivers already configured.\n",
    "\n",
    "2. No need to manually set up GPU drivers or CUDA versions — less time debugging environment issues.\n",
    "\n",
    "\n",
    "### Easy collaboration & sharing\n",
    "\n",
    "1. You can share a Colab notebook via link (like Google Docs).\n",
    "\n",
    "2. Students can run the exact same code in the same environment without setup headaches.\n",
    "\n",
    "### Cloud compute = no local resource drain\n",
    "\n",
    "1. Heavy model inference/training doesn’t slow down your personal laptop.\n",
    "\n",
    "2. Works even on lightweight machines like Chromebooks.\n",
    "\n",
    "### Zero-cost or low-cost experiments\n",
    "\n",
    "Free tier for small experiments; paid tiers for bigger models.\n",
    "\n",
    "You can scale up to larger GPUs only when needed.\n",
    "\n",
    "\n",
    "## Key Differences: Google Colab vs. Local Jupyter Notebook\n",
    "\n",
    "| Feature | Google Colab | Local Jupyter Notebook |\n",
    "|---------|--------------|------------------------|\n",
    "| **Hardware** | Remote cloud CPU/GPU/TPU | Local CPU/GPU only |\n",
    "| **GPU Access** | Yes (free or paid tiers) | Only if your machine has a GPU |\n",
    "| **Setup Time** | Minimal — pre-installed AI libs | Manual — need to install Python libs, drivers |\n",
    "| **Persistence** | Temporary runtime (files gone after session ends) | Permanent local storage |\n",
    "| **Collaboration** | Easy sharing via link | Must send files / run on same network |\n",
    "| **Speed** | Depends on Colab's assigned hardware | Depends on your local hardware |\n",
    "| **Internet Dependency** | Requires constant internet | Can run offline |\n",
    "| **Privacy** | Code/data stored in Google's servers | Fully under your control locally |\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af0de05c",
   "metadata": {},
   "source": [
    "# 1. PyTorch — “The engine under the hood”\n",
    "\n",
    "What it is:\n",
    "\n",
    "1. An open-source machine learning framework.\n",
    "\n",
    "2. Lets you build and train neural networks.\n",
    "\n",
    "3. Handles the math (tensors, gradients) and runs on CPU or GPU.\n",
    "\n",
    "Think of it like: The engine in a car — powerful but not very user-friendly for casual drivers. Most people use a higher-level interface (like Hugging Face) built on top of it.\n",
    "\n",
    "Use case in LLMs:\n",
    "\n",
    "PyTorch runs the actual math for model inference/training when you load LLaMA, GPT-J, Mistral, etc.\n",
    "\n",
    "\n",
    "# 2. Hugging Face Transformers — “The model library”\n",
    "\n",
    "What it is:\n",
    "\n",
    "1. A Python library with pre-trained models (text, image, audio, multimodal).\n",
    "\n",
    "2. Saves you from coding neural networks from scratch.\n",
    "\n",
    "3. Works on top of PyTorch or TensorFlow.\n",
    "\n",
    "Think of it like: Netflix for AI models — you choose what you want and start using it right away.\n",
    "\n",
    "Example: Load a sentiment analysis model\n",
    "\n",
    "```python\n",
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"sentiment-analysis\")\n",
    "print(classifier(\"I love learning about LLMs!\"))\n",
    "# [{'label': 'POSITIVE', 'score': 0.9998}]\n",
    "```\n",
    "\n",
    "Use case in LLMs:\n",
    "\n",
    "You can download and run open-source LLMs (e.g., Mistral, Qwen, LLaMA) with a few lines of code.\n",
    "\n",
    "\n",
    "# 3. Pipeline API — “The shortcut”\n",
    "\n",
    "What it is:\n",
    "\n",
    "A high-level Hugging Face function that bundles:\n",
    "\n",
    "1. Model\n",
    "\n",
    "2. Tokenizer\n",
    "\n",
    "3. Preprocessing\n",
    "\n",
    "4. Postprocessing\n",
    "\n",
    "Lets you use a model without worrying about the internal steps.\n",
    "\n",
    "Think of it like: A coffee machine — you just press a button instead of manually grinding beans, boiling water, etc.\n",
    "\n",
    "Example: Text generation pipeline\n",
    "\n",
    "```python\n",
    "from transformers import pipeline\n",
    "\n",
    "generator = pipeline(\"text-generation\", model=\"gpt2\")\n",
    "print(generator(\"Once upon a time\", max_length=20))\n",
    "```\n",
    "\n",
    "Use case in LLMs:\n",
    "\n",
    "For quick experiments — perfect for classroom demos and small prototypes.\n",
    "\n",
    "# 4. Tokenizer API — “The language converter”\n",
    "\n",
    "What it is:\n",
    "\n",
    "1, Converts human text into tokens (numbers) the model understands.\n",
    "\n",
    "2, Also converts model output (tokens) back into human-readable text.\n",
    "\n",
    "Think of it like: Google Translate — but translating English → model-language (numbers).\n",
    "\n",
    "Example:\n",
    "```python\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "text = \"Hello world\"\n",
    "tokens = tokenizer.encode(text)\n",
    "print(tokens)         # [15496, 995]\n",
    "print(tokenizer.decode(tokens))  # \"Hello world\"\n",
    "```\n",
    "\n",
    "Use case in LLMs:\n",
    "\n",
    "Every time you send a prompt, it’s first tokenized before going into the model — crucial for understanding context length & token limits.\n",
    "\n",
    "### Real-world LLM workflow\n",
    "\n",
    "Here’s how they work together when running an open-source LLM:\n",
    "\n",
    "1. PyTorch → Does the heavy math on CPU/GPU.\n",
    "\n",
    "2. Hugging Face Transformers → Gives you the model weights and architecture.\n",
    "\n",
    "3. Tokenizer → Turns text into tokens and back.\n",
    "\n",
    "4. Pipeline → Simplifies the process so you don’t have to manually wire everything."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c87d2607",
   "metadata": {},
   "source": [
    "# What is Quantization?\n",
    "\n",
    "Quantization is a technique that reduces the precision of model weights (from 32-bit or 16-bit to 4-bit) to save memory and speed up inference while maintaining reasonable accuracy.\n",
    "\n",
    "Code Explanations:\n",
    "\n",
    "```python\n",
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,                    # Enable 4-bit quantization\n",
    "    bnb_4bit_use_double_quant=True,       # Use double quantization for extra memory savings\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16, # Data type for computations\n",
    "    bnb_4bit_quant_type=\"nf4\"            # Type of 4-bit quantization\n",
    ")\n",
    "```\n",
    "Parameter Details:\n",
    "\n",
    "### 1. load_in_4bit=True\n",
    "\n",
    "- **Purpose:** Enables 4-bit quantization when loading the model\n",
    "- **Effect:** Reduces model memory usage by ~75% (from 16-bit to 4-bit)\n",
    "- **Trade-off:** Slight accuracy loss for significant memory savings\n",
    "\n",
    "### 2. bnb_4bit_use_double_quant=True\n",
    "\n",
    "- **Purpose:** Uses double quantization technique\n",
    "- **Effect:** Further reduces memory usage by ~10-15%\n",
    "- **How it works:** Quantizes the quantization scales themselves\n",
    "- **Benefit:** More memory efficient without significant performance loss\n",
    "\n",
    "### 3. bnb_4bit_compute_dtype=torch.bfloat16\n",
    "\n",
    "- **Purpose:** Specifies the data type for computations\n",
    "- **Choice:** bfloat16 (Brain Floating Point 16-bit)\n",
    "- **Benefits:**\n",
    "  - Better numerical stability than float16\n",
    "  - Faster than float32\n",
    "  - Good balance between precision and speed\n",
    "\n",
    "### 4. bnb_4bit_quant_type=\"nf4\"\n",
    "\n",
    "- **Purpose:** Specifies the type of 4-bit quantization\n",
    "- **Option:** \"nf4\" (NormalFloat4)\n",
    "- **Benefits:**\n",
    "  - Better accuracy than standard 4-bit quantization\n",
    "  - Optimized for normal distributions (common in neural networks)\n",
    "  - More efficient representation of weight values\n",
    "\n",
    "\n",
    "\n",
    "### When to Use:\n",
    "\n",
    "✅ Use 4-bit quantization when:\n",
    "\n",
    "1. Limited GPU memory (e.g., 8GB or less)\n",
    "\n",
    "2. Need to load large models on consumer hardware\n",
    "\n",
    "3. Batch processing where memory efficiency matters\n",
    "\n",
    "4. Prototyping or development phases\n",
    "\n",
    "❌ Avoid when:\n",
    "\n",
    "1. Maximum accuracy is required\n",
    "\n",
    "2. Sufficient GPU memory is available (32GB+)\n",
    "\n",
    "3. Production deployments where accuracy is critical\n",
    "\n",
    "Performance Impact:\n",
    "\n",
    "1. Memory usage: Reduced by ~75%\n",
    "\n",
    "2. Speed: Similar or slightly faster\n",
    "\n",
    "3. Accuracy: Typically 1-3% degradation\n",
    "\n",
    "4. Compatibility: Works with most transformer models\n",
    "\n",
    "This configuration is particularly useful for running large language models on consumer GPUs or when you need to load multiple models simultaneously.\n",
    "\n",
    "### Memory Savings Comparison\n",
    "\n",
    "| Precision | Memory Usage | Accuracy | Speed |\n",
    "|-----------|--------------|----------|-------|\n",
    "| 32-bit (float32) | 100% | Best | Slow |\n",
    "| 16-bit (float16) | 50% | Good | Fast |\n",
    "| 4-bit (quantized) | 25% | Acceptable | Fastest |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0cff5c7",
   "metadata": {},
   "source": [
    "### Code Explanations:\n",
    "\n",
    "```python\n",
    "tokenizer = AutoTokenizer.from_pretrained(LLAMA)    # 1. Load the tokenizer for the Llama model\n",
    "tokenizer.pad_token = tokenizer.eos_token   # 2. Set padding token to end-of-sequence token\n",
    "inputs = tokenizer.apply_chat_template(messages, return_tensors=\"pt\").to(\"cuda\")  # 3. Convert messages to tensor format and move to GPU\n",
    "streamer = TextStreamer(tokenizer)   # 4. Create a text streamer for real-time output\n",
    "model = AutoModelForCausalLM.from_pretrained(LLAMA, device_map=\"auto\", quantization_config=quant_config)  # 5. Load the model with quantization and auto device mapping\n",
    "outputs = model.generate(inputs, max_new_tokens=2000, streamer=streamer)  # 6. Generate text with streaming output\n",
    "```\n",
    "\n",
    "Detailed Breakdown:\n",
    "\n",
    "```python\n",
    "tokenizer = AutoTokenizer.from_pretrained(LLAMA)\n",
    "```\n",
    "- **Purpose:** Loads the tokenizer associated with the Llama model\n",
    "- **What it does:** Converts text to numerical tokens that the model can understand\n",
    "- **Example:** `\"Hello world\"` → `[1, 15043, 2787]` (token IDs)\n",
    "\n",
    "```python\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "```\n",
    "\n",
    "- **Purpose:** Sets the padding token to be the same as the end-of-sequence token\n",
    "- **Why needed:** Ensures consistent padding across different input lengths\n",
    "- **Common issue:** Many tokenizers don't have a pad_token by default\n",
    "\n",
    "```python\n",
    "inputs = tokenizer.apply_chat_template(messages, return_tensors=\"pt\").to(\"cuda\")\n",
    "```\n",
    "- **apply_chat_template():** Formats messages into the model's expected chat format\n",
    "- **return_tensors=\"pt\":** Returns PyTorch tensors\n",
    "- **.to(\"cuda\"):** Moves tensors to GPU for faster processing\n",
    "\n",
    "```python\n",
    "streamer = TextStreamer(tokenizer)\n",
    "```\n",
    "\n",
    "- **Purpose:** Creates a streamer for real-time text output\n",
    "- **Benefit:** Shows generated text as it's being created (like ChatGPT)\n",
    "- **Alternative:** Without streaming, you'd wait for the entire response\n",
    "\n",
    "\n",
    "```python\n",
    "model = AutoModelForCausalLM.from_pretrained(...)\n",
    "```\n",
    "\n",
    "- **AutoModelForCausalLM:** Loads a causal language model (predicts next tokens)\n",
    "- **device_map=\"auto\":** Automatically distributes model across available devices\n",
    "- **quantization_config=quant_config:** Applies 4-bit quantization for memory efficiency\n",
    "\n",
    "```python\n",
    "model.generate(inputs, max_new_tokens=2000, streamer=streamer)\n",
    "```\n",
    "\n",
    "- **generate():** Starts the text generation process\n",
    "- **max_new_tokens=2000:** Limits output to 2000 new tokens\n",
    "- **streamer=streamer:** Enables real-time streaming of generated text\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
