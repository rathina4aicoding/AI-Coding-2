{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2469154f",
   "metadata": {},
   "source": [
    "# Conversational AI - aka Chatbot!\n",
    "\n",
    "Adding history of past conversations and Context or Prompt Enrichments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "540a6e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import os\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "from bs4 import BeautifulSoup\n",
    "from IPython.display import Markdown, display, update_display\n",
    "#from IPython.display import Markdown, display\n",
    "from openai import OpenAI\n",
    "import json\n",
    "from typing import List\n",
    "import anthropic\n",
    "import gradio as gr\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d1751ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables in a file called .env\n",
    "# Print the key prefixes to help with any debugging\n",
    "\n",
    "load_dotenv(override=True)\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "anthropic_api_key = os.getenv('ANTHROPIC_API_KEY')\n",
    "google_api_key = os.getenv('GOOGLE_API_KEY')\n",
    "\n",
    "if openai_api_key:\n",
    "    print(f\"OpenAI API Key exists and begins {openai_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"OpenAI API Key not set\")\n",
    "    \n",
    "if anthropic_api_key:\n",
    "    print(f\"Anthropic API Key exists and begins {anthropic_api_key[:7]}\")\n",
    "else:\n",
    "    print(\"Anthropic API Key not set\")\n",
    "\n",
    "if google_api_key:\n",
    "    print(f\"Google API Key exists and begins {google_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"Google API Key not set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b333dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize\n",
    "\n",
    "openai = OpenAI()\n",
    "claude = anthropic.Anthropic()\n",
    "MODEL = 'gpt-4o-mini'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3127e65a",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = \"You are a helpful assistant\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d22cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We will write a function `chat(message, history)` where:  \n",
    "#**message** is the prompt to use  \n",
    "#**history** is the past conversation, in OpenAI format  \n",
    "#We will combine the system message, history and latest message, then call OpenAI.\n",
    "\n",
    "\n",
    "def chat(message, history):\n",
    "    #messages = [{\"role\": \"system\", \"content\": system_message}] + history + [{\"role\": \"user\", \"content\": message}]\n",
    "    messages = [{\"role\": \"system\", \"content\": system_message},\n",
    "                {\"role\": \"user\", \"content\": message}]\n",
    "\n",
    "    #print(\"History is:\")\n",
    "    #print(history)\n",
    "    print(\"And messages is:\")\n",
    "    print(messages)\n",
    "\n",
    "    stream = openai.chat.completions.create(model=MODEL, messages=messages, stream=True)\n",
    "\n",
    "    response = \"\"\n",
    "    for chunk in stream:\n",
    "        response += chunk.choices[0].delta.content or ''\n",
    "        yield response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed066d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "gr.ChatInterface(fn=chat, type=\"messages\").launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33993c91",
   "metadata": {},
   "source": [
    "# üß† How Memory Works in Chatbots\n",
    "\n",
    "In conversational chatbots, memory is usually simulated by passing past interactions back into the model.\n",
    "\n",
    "LLMs themselves are stateless ‚Üí they don‚Äôt remember beyond a single request.\n",
    "\n",
    "To give the illusion of memory, developers store past conversation history (in a list, DB, or vector store).\n",
    "\n",
    "Each new API call includes this history as context so the model can generate a consistent, coherent reply.\n",
    "\n",
    "üëâ In short: memory is ‚Äúfed back in,‚Äù not ‚Äúremembered natively.‚Äù"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d1a870e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Context enrichment\n",
    "system_message = \"You are a helpful assistant in a clothes store. You should try to gently encourage \\\n",
    "the customer to try items that are on sale. Hats are 60% off, and most other items are 50% off. \\\n",
    "For example, if the customer says 'I'm looking to buy a hat', \\\n",
    "you could reply something like, 'Wonderful - we have lots of hats - including several that are part of our sales event.'\\\n",
    "Encourage the customer to buy hats if they are unsure what to get.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2139354",
   "metadata": {},
   "source": [
    "## Conversation Replay or Context-Window Memory or Na√Øve Chat Memory or Buffer Memory\n",
    "### Example:\n",
    "```\n",
    "messages = [{\"role\": \"system\", \"content\": system_message}] + history + [{\"role\": \"user\", \"content\": message}]\n",
    "\n",
    "```\n",
    "\n",
    "How it works: Store all past messages ‚Üí concatenate ‚Üí send back with each request.\n",
    "\n",
    "Pros: Simple, coherent, no extra infra.\n",
    "\n",
    "Cons: Grows fast, hits token/context limits, gets expensive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c3e8a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(message, history):\n",
    "    messages = [{\"role\": \"system\", \"content\": system_message}] + history + [{\"role\": \"user\", \"content\": message}]\n",
    "\n",
    "    stream = openai.chat.completions.create(model=MODEL, messages=messages, stream=True)\n",
    "\n",
    "    response = \"\"\n",
    "    for chunk in stream:\n",
    "        response += chunk.choices[0].delta.content or ''\n",
    "        yield response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f3d1d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "gr.ChatInterface(fn=chat, type=\"messages\").launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a2d3c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message += \"\\nIf the customer asks for shoes, you should respond that shoes are not on sale today, \\\n",
    "but remind the customer to look at hats!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a85ebfcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prompt Chaining\n",
    "print(system_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df17345d",
   "metadata": {},
   "outputs": [],
   "source": [
    "gr.ChatInterface(fn=chat, type=\"messages\").launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ebfb7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I've also improved the structure of this function\n",
    "\n",
    "def chat(message, history):\n",
    "\n",
    "    relevant_system_message = system_message\n",
    "    if 'belt' in message:\n",
    "        relevant_system_message += \" The store does not sell belts; if you are asked for belts, be sure to point out other items on sale.\"\n",
    "    \n",
    "    \n",
    "    messages = [{\"role\": \"system\", \"content\": relevant_system_message}] + history + [{\"role\": \"user\", \"content\": message}]\n",
    "\n",
    "    stream = openai.chat.completions.create(model=MODEL, messages=messages, stream=True)\n",
    "\n",
    "    response = \"\"\n",
    "    for chunk in stream:\n",
    "        response += chunk.choices[0].delta.content or ''\n",
    "        yield response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60195431",
   "metadata": {},
   "outputs": [],
   "source": [
    "gr.ChatInterface(fn=chat, type=\"messages\").launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
