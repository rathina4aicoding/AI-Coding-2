{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d14741a6",
   "metadata": {},
   "source": [
    "# YOUR FIRST LAB\n",
    "## Your first Frontier LLM Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97034882",
   "metadata": {},
   "source": [
    "## Setting up your keys\n",
    "\n",
    "If you haven't done so already, you could now create API keys for Anthropic and Google in addition to OpenAI.\n",
    "\n",
    "**Please note:** if you'd prefer to avoid extra API costs, feel free to skip setting up Anthopic and Google! You can see me do it, and focus on OpenAI for the course. You could also substitute Anthropic and/or Google for Ollama, using the exercise you did in week 1.\n",
    "\n",
    "For OpenAI, visit https://openai.com/api/  \n",
    "For Anthropic, visit https://console.anthropic.com/  \n",
    "For Google, visit https://ai.google.dev/gemini-api  \n",
    "\n",
    "### Also - adding DeepSeek if you wish\n",
    "\n",
    "Optionally, if you'd like to also use DeepSeek, create an account [here](https://platform.deepseek.com/), create a key [here](https://platform.deepseek.com/api_keys) and top up with at least the minimum $2 [here](https://platform.deepseek.com/top_up).\n",
    "\n",
    "### Adding API keys to your .env file\n",
    "\n",
    "When you get your API keys, you need to set them as environment variables by adding them to your `.env` file.\n",
    "\n",
    "```\n",
    "OPENAI_API_KEY=xxxx\n",
    "ANTHROPIC_API_KEY=xxxx\n",
    "GOOGLE_API_KEY=xxxx\n",
    "DEEPSEEK_API_KEY=xxxx\n",
    "```\n",
    "\n",
    "Afterwards, you may need to restart the Jupyter Lab Kernel (the Python process that sits behind this notebook) via the Kernel menu, and then rerun the cells from the top."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "353b32e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import os\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "from bs4 import BeautifulSoup\n",
    "from IPython.display import Markdown, display\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b7cfc07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API key found and looks good so far!\n"
     ]
    }
   ],
   "source": [
    "# Load environment variables in a file called .env\n",
    "\n",
    "load_dotenv(override=True)\n",
    "api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "# Check the key\n",
    "#check if the key is valid\n",
    "\n",
    "if not api_key:\n",
    "    print(\"No API key was found - please head over to the troubleshooting notebook in this folder to identify & fix!\")\n",
    "elif not api_key.startswith(\"sk-proj-\"):\n",
    "    print(\"An API key was found, but it doesn't start sk-proj-; please check you're using the right key - see troubleshooting notebook\")\n",
    "elif api_key.strip() != api_key:\n",
    "    print(\"An API key was found, but it looks like it might have space or tab characters at the start or end - please remove them - see troubleshooting notebook\")\n",
    "else:\n",
    "    print(\"API key found and looks good so far!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b572bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# craete a openai client \n",
    "openai = OpenAI()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58c6b32c",
   "metadata": {},
   "source": [
    "# Let's make a quick call to a Frontier model to get started, as a preview!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d472fa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! It’s great to see you reaching out through Python code! If you have any questions, need assistance with code, or just want to chat, feel free to ask. How can I help you today?\n"
     ]
    }
   ],
   "source": [
    "# Calling OpenAI Model with a prompt\n",
    "message = \"Hello, GPT! This is my first ever message to you via  a python code! Hi!\"\n",
    "response = openai.chat.completions.create(model=\"gpt-4o-mini\", messages=[{\"role\":\"user\", \"content\":message}])\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cfb2929",
   "metadata": {},
   "source": [
    "## Recap on installation of Ollama. Ollama is a Open Source Model with no API Cost!!\n",
    "\n",
    "Simply visit [ollama.com](https://ollama.com) and install!\n",
    "\n",
    "Once complete, the ollama server should already be running locally.  \n",
    "If you visit:  \n",
    "[http://localhost:11434/](http://localhost:11434/)\n",
    "\n",
    "You should see the message `Ollama is running`.  \n",
    "\n",
    "If not, bring up a new Terminal (Mac) or Powershell (Windows) and enter `ollama serve`  \n",
    "And in another Terminal (Mac) or Powershell (Windows), enter `ollama pull llama3.2`  \n",
    "Then try [http://localhost:11434/](http://localhost:11434/) again.\n",
    "\n",
    "If Ollama is slow on your machine, try using `llama3.2:1b` as an alternative. Run `ollama pull llama3.2:1b` from a Terminal or Powershell, and change the code below from `MODEL = \"llama3.2\"` to `MODEL = \"llama3.2:1b\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "745e648a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "23234822",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "\n",
    "OLLAMA_API = \"http://localhost:11434/api/chat\"\n",
    "HEADERS = {\"Content-Type\": \"application/json\"}\n",
    "MODEL = \"llama3.2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6675172c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a messages list using the same format that we used for OpenAI\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"What are the top Agentic AI frameworks used in banking and financial sectors?\"}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf20cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "payload = {\n",
    "        \"model\": MODEL,\n",
    "        \"messages\": messages,\n",
    "        \"stream\": False\n",
    "          \n",
    " }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "610eb237",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's just make sure the model is loaded\n",
    "\n",
    "!ollama pull llama3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6ba62aea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the banking and financial sectors, several Agentic AI frameworks have been adopted to enhance decision-making, process automation, and customer engagement. Here are some of the top Agentic AI frameworks used in these industries:\n",
      "\n",
      "1. **Google Cloud's AutoML**: Google Cloud's AutoML is a popular choice for automating machine learning workflows in banking and finance. It provides pre-built algorithms and templates to build predictive models, natural language processing (NLP) models, and computer vision models.\n",
      "2. **Microsoft Cognitive Services**: Microsoft Cognitive Services offers a range of Agentic AI frameworks, including Computer Vision, Natural Language Processing, and Speech Recognition. These services are widely used in banking and finance for tasks like image recognition, text analysis, and sentiment analysis.\n",
      "3. **IBM Watson**: IBM Watson is a cloud-based platform that leverages natural language processing (NLP), machine learning, and artificial intelligence to analyze large amounts of data. It's commonly used in banking and finance for tasks like customer service chatbots, risk management, and portfolio optimization.\n",
      "4. **SAP Leonardo**: SAP Leonardo is an open-source platform that enables organizations to build digital business models using Agentic AI techniques. It provides a range of tools and services for data integration, machine learning, and IoT connectivity.\n",
      "5. **Apache Kafka + Strimzi**: Apache Kafka is a popular messaging platform used in banking and finance for event-driven processing. Strimzi is a commercial offering that simplifies the deployment and management of Kafka clusters, enabling real-time data processing and analytics.\n",
      "6. **TensorFlow with Google Cloud AI Platform**: TensorFlow is an open-source machine learning framework developed by Google. When combined with Google Cloud AI Platform, it provides a scalable and managed environment for building and deploying Agentic AI models in banking and finance.\n",
      "7. **AWS SageMaker**: AWS SageMaker is a fully managed service offered by Amazon Web Services (AWS) that simplifies the process of building, training, and deploying machine learning models using Agentic AI techniques. It's widely used in banking and finance for tasks like predictive modeling and natural language processing.\n",
      "\n",
      "These frameworks are commonly used in banking and financial sectors to build:\n",
      "\n",
      "* Predictive models for credit risk assessment and portfolio optimization\n",
      "* Natural language processing models for customer service chatbots and sentiment analysis\n",
      "* Computer vision models for image recognition and facial analysis\n",
      "* Machine learning models for fraud detection and anomaly identification\n",
      "* IoT-enabled systems for monitoring market trends and sentiment analysis\n",
      "\n",
      "It's worth noting that the choice of Agentic AI framework depends on the specific use case, industry requirements, and organizational preferences.\n"
     ]
    }
   ],
   "source": [
    "response = requests.post(OLLAMA_API, json=payload, headers=HEADERS)\n",
    "print(response.json()['message']['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac1eb0fa",
   "metadata": {},
   "source": [
    "# Introducing the ollama package\n",
    "\n",
    "And now we'll do the same thing, but using the elegant ollama python package instead of a direct HTTP call.\n",
    "\n",
    "Under the hood, it's making the same call as above to the ollama server running at localhost:11434"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bce187e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the banking and financial sectors, several agentic AI frameworks have been adopted to enhance decision-making and automate processes. Here are some of the top agentic AI frameworks used in these industries:\n",
      "\n",
      "1. **IBM Watson**: IBM's Watson platform is widely used in banking and finance for various applications such as customer service, risk management, and compliance monitoring.\n",
      "2. **Microsoft Azure Machine Learning**: Microsoft's Azure Machine Learning (AML) framework provides a comprehensive set of tools for building, deploying, and managing machine learning models in the banking sector.\n",
      "3. **Google Cloud AI Platform**: Google Cloud AI Platform offers a suite of services for building, deploying, and managing machine learning models, including those used in banking and financial applications.\n",
      "4. **Apache MXNet**: Apache MXNet is an open-source deep learning framework that has gained significant traction in the banking industry for tasks such as risk assessment and credit scoring.\n",
      "5. **TensorFlow**: TensorFlow is another popular open-source machine learning framework developed by Google, widely adopted in banking and finance for various applications like fraud detection and portfolio optimization.\n",
      "6. **RapidMiner**: RapidMiner is a business analytics platform that provides a range of tools for data preparation, feature engineering, model selection, and deployment, commonly used in the financial sector.\n",
      "7. **H2O.ai Driverless AI**: H2O.ai's Driverless AI is an automated machine learning platform designed to simplify the process of building predictive models in banking and finance applications.\n",
      "\n",
      "These frameworks are often used in combination with other technologies such as natural language processing (NLP), computer vision, and predictive analytics to drive business decisions and improve operational efficiency in the banking sector.\n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "\n",
    "MODEL = \"llama3.2\"\n",
    "response = ollama.chat(model=MODEL, messages=messages)\n",
    "print(response['message']['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "995c61a7",
   "metadata": {},
   "source": [
    "## Alternative approach - using OpenAI python library to connect to Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "27864fe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the banking and financial sectors, there are several agnostic AI (Artificial Intelligence) frameworks that can be leveraged to enhance various business processes. Here are some of the top agentic AI frameworks commonly used:\n",
      "\n",
      "1. **Microsoft Power Automate (formerly Microsoft Flow)**: A hybrid workflow automation platform that enables users to create intelligent pipelines for automating business tasks, such as approval processes, data integration, and workflows.\n",
      "2. **Alteryx**: A self-service analytics platform that allows users to create data analytics workbooks, which can be used for predictive modeling, data visualization, and machine learning tasks.\n",
      "3. **IBM Watson Analytics**: A cloud-based analytics platform that supports a wide range of AI and machine learning algorithms for predictive analytics, data science, and business intelligence.\n",
      "4. **Kubeflow**: An open-source machine learning (ML) framework for Apache Spark, which enables users to develop and deploy ML models in Azure, AWS, or on-premises environments.\n",
      "5. **Google Cloud AI Platform**: A fully managed platform that offers a range of agnostic AI frameworks, including TensorFlow, PyTorch, and OpenCV, for developers to build, deploy, and manage their AI solutions.\n",
      "6. **Tableau Connect**: An enterprise-level data integration and analytics platform that supports various agnostic AI tools, such as Salesforce, NetSuite, and SAP, for automating business processes.\n",
      "7. **Dremio**: A unified schema-on-read architecture-based data warehousing and ETL (Extract, Transform, Load) toolset for big data platforms like Apache Spark, AWS Glue, or Azure Data Factory.\n",
      "8. **Ansible Automation Hub**: An open-source automation platform that supports various agnostic AI tools, including machine learning, natural language processing (NLP), and computer vision, to build automated workflows.\n",
      "\n",
      "These frameworks are widely used across the banking and financial sectors for implementing agnostic AI solutions in areas such as:\n",
      "\n",
      "* Predictive analytics\n",
      "* Machine learning\n",
      "* Data integration and ETL\n",
      "* Automation of business processes\n",
      "* Analytics and reporting\n",
      "\n",
      "However, it's essential to note that some businesses might prefer to keep their AI infrastructure agnostic to ensure flexibility and adaptability across different applications, platforms, or regions.\n"
     ]
    }
   ],
   "source": [
    "# You can use the OpenAI client python library to call Ollama:\n",
    "\n",
    "MODEL = \"llama3.2:1b\"\n",
    "\n",
    "from openai import OpenAI\n",
    "ollama_via_openai = OpenAI(base_url='http://localhost:11434/v1', api_key='ollama')\n",
    "\n",
    "response = ollama_via_openai.chat.completions.create(\n",
    "    model=MODEL,\n",
    "    messages=messages\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cbd3c81",
   "metadata": {},
   "source": [
    "\n",
    "Here we use the version of DeepSeek-reasoner that's been distilled to 1.5B.  \n",
    "This is actually a 1.5B variant of Qwen that has been fine-tuned using synethic data generated by Deepseek R1.\n",
    "\n",
    "Other sizes of DeepSeek are [here](https://ollama.com/library/deepseek-r1) all the way up to the full 671B parameter version, which would use up 404GB of your drive and is far too large for most!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "278ecd4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[?25lpulling manifest ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling aabd4debf0c8... 100% ▕████████████████▏ 1.1 GB                         \n",
      "pulling c5ad996bda6e... 100% ▕████████████████▏  556 B                         \n",
      "pulling 6e4c38e1172f... 100% ▕████████████████▏ 1.1 KB                         \n",
      "pulling f4d24e9138dd... 100% ▕████████████████▏  148 B                         \n",
      "pulling a85fe2a2e58e... 100% ▕████████████████▏  487 B                         \n",
      "verifying sha256 digest \n",
      "writing manifest \n",
      "success \u001b[?25h\n"
     ]
    }
   ],
   "source": [
    "!ollama pull deepseek-r1:1.5b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f7f4298d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, so I need to figure out how to explain a few key concepts behind LLMs in simple terms. The user mentioned neural networks, attention, and transformers. Let me start by breaking down each one.\n",
      "\n",
      "First, I know that neural networks are like the brain of machines, right? They process data through layers of nodes or neurons, with hidden layers where information is handled. In an LLM, those layers probably handle language tasks because they're designed to understand and generate human-like text. Maybe like how an AI knows when to think and where to speak.\n",
      "\n",
      "Then, attention. From what I recall, when I'm listening to someone talk, my brain doesn't look at every word in the sentence right away. It focuses on what's relevant and ignores the rest. So in LLMs,attention must be about focusing on specific parts of text or data, giving more weight to important pieces while ignoring less critical ones dynamically.\n",
      "\n",
      "The Transformer architecture caught me initially because I've heard a lot about it being good for understanding context because of something called self-attention. But how does that work? From what I remember, instead of processing each word one by one like an RNN might do, the Transformer processes the data in parallel across all positions or tokens. This means it can capture more global dependencies between elements, which is useful for language understanding without relying on sequential order.\n",
      "\n",
      "Putting it together, a neural network in an LLM would take raw input (like text) through this structured system of layers and attention mechanisms to understand the context and generate outputs that are semantically meaningful. The Transformers might add the ability to process massive sequences efficiently by using self-attention across different contexts at their own times.\n",
      "\n",
      "Wait, so maybe I should explain each part step by step without being too technical. Make sure it's clear what each component does in simpler language so someone new can grasp them. Also, emphasize how these components work together for an LLM to perform tasks like text generation or classification.\n",
      "\n",
      "I'm a bit fuzzy on the exact functions of attention and Transformers yet, but I think that self-attention allows the model to consider all possible relationships between data points simultaneously, which is something RNNs or even simpler models might not achieve as effectively. It's this ability that enables the model to understand context better by considering various contextual windows dynamically.\n",
      "\n",
      "I should also mention how these components make LLMs capable of handling large amounts of data and complex patterns without requiring sequential processing step by step. This seems to be a key strength, which makes them powerful for various tasks like mathematics solving, creative writing, etc.\n",
      "\n",
      "Hmm, I think I've got the basics. I just need to structure these thoughts into clear definitions, maybe using simple analogies or examples they can relate to, and make sure each component is clearly explained without getting too deep into the math or implementation details.\n",
      "</think>\n",
      "\n",
      "Certainly! Let me break down the key concepts behind Large Language Models (LLMs) in a way that's easy to understand:\n",
      "\n",
      "1. **Neural Networks (or AI Brains):**\n",
      "   - **Function:** Neural networks are the \"brain\" of machine learning systems, inspired by biological neural structures. They process data through layers of nodes or neurons.\n",
      "   - **How They Work with LLMs:** In these models, each neuron processes input in a layered structure with hidden layers where data flows and is handled. The hidden layers handle tasks like understanding grammar or context in the data.\n",
      "\n",
      "2. **Attention:**\n",
      "   - **Function:** Attention allows machines to focus on specific parts of the data relevant to the task at hand. It's about paying attention to important elements while ignoring less crucial ones.\n",
      "   - **Dynamic Focus:** In LLMs, attention is dynamic, shifting focus across the text or data as needed. For example, it might focus on key facts first before moving on to details.\n",
      "\n",
      "3. **Transformer Architecture:**\n",
      "   - **Function:** The Transformer processes sequences of data in layers without relying solely on sequential order. It does this by processing information globally, leveraging self-attention.\n",
      "   - **Self-Attention for Context:** Unlike RNNs, which process each word step by step, Transformers process data across all positions simultaneously or in parallel. This allows them to understand contexts and relationships more holistically.\n",
      "\n",
      "These components work together in LLMs to understand context dynamically, generate outputs that are semantically meaningful, and handle large amounts of complex patterns efficiently without sequential processing.\n"
     ]
    }
   ],
   "source": [
    "# This may take a few minutes to run! You should then see a fascinating \"thinking\" trace inside <think> tags, followed by some decent definitions\n",
    "\n",
    "response = ollama_via_openai.chat.completions.create(\n",
    "    model=\"deepseek-r1:1.5b\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Please give definitions of some core concepts behind LLMs: a neural network, attention and the transformer\"}]\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
